{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fd22fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bodhi\\miniconda3\\envs\\machine learning in CS\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\bodhi\\miniconda3\\envs\\machine learning in CS\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import random\n",
    "\n",
    "from six import string_types\n",
    "import torch.optim as optim\n",
    "# mister_ed\n",
    "import recoloradv.mister_ed.loss_functions as lf \n",
    "import recoloradv.mister_ed.utils.pytorch_utils as utils\n",
    "import recoloradv.mister_ed.utils.image_utils as img_utils\n",
    "import recoloradv.mister_ed.cifar10.cifar_loader as cifar_loader\n",
    "import recoloradv.mister_ed.cifar10.cifar_resnets as cifar_resnets\n",
    "import recoloradv.mister_ed.adversarial_training as advtrain\n",
    "import recoloradv.mister_ed.utils.checkpoints as checkpoints\n",
    "import recoloradv.mister_ed.adversarial_perturbations as ap \n",
    "import recoloradv.mister_ed.adversarial_attacks as aa\n",
    "import recoloradv.mister_ed.spatial_transformers as st\n",
    "import recoloradv.mister_ed.config as config\n",
    "\n",
    "# ReColorAdv\n",
    "import recoloradv.perturbations as pt\n",
    "import recoloradv.color_transformers as ct\n",
    "import recoloradv.color_spaces as cs\n",
    "from recoloradv import norms\n",
    "from recoloradv.utils import load_pretrained_cifar10_model, get_attack_from_name\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87e42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## resnet model\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    import numpy as np\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277392ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attack algorithm (nothing changes functionswise but i edit it to keep the images it produces)\n",
    "\n",
    "class AdversarialAttack(object):\n",
    "    \"\"\" Wrapper for adversarial attacks. Is helpful for when subsidiary methods\n",
    "        are needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classifier_net, normalizer, threat_model,\n",
    "                 manual_gpu=None):\n",
    "        \"\"\" Initializes things to hold to perform a single batch of\n",
    "            adversarial attacks\n",
    "        ARGS:\n",
    "            classifier_net : nn.Module subclass - neural net that is the\n",
    "                             classifier we're attacking\n",
    "            normalizer : DifferentiableNormalize object - object to convert\n",
    "                         input data to mean-zero, unit-var examples\n",
    "            threat_model : ThreatModel object - object that allows us to create\n",
    "                           per-minibatch adversarial examples\n",
    "            manual_gpu : None or boolean - if not None, we override the\n",
    "                         environment variable 'MISTER_ED_GPU' for how we use\n",
    "                         the GPU in this object\n",
    "        \"\"\"\n",
    "        self.classifier_net = classifier_net\n",
    "#         self.normalizer = normalizer or utils.IdentityNormalize()\n",
    "        self.normalizer = utils.IdentityNormalize()\n",
    "        if manual_gpu is not None:\n",
    "            self.use_gpu = manual_gpu\n",
    "        else:\n",
    "            self.use_gpu = utils.use_gpu()\n",
    "        self.validator = lambda *args: None\n",
    "        self.threat_model = threat_model\n",
    "\n",
    "    @property\n",
    "    def _dtype(self):\n",
    "        return torch.cuda.FloatTensor if self.use_gpu else torch.FloatTensor\n",
    "\n",
    "    def setup(self):\n",
    "        self.classifier_net.eval()\n",
    "        self.normalizer.differentiable_call()\n",
    "\n",
    "\n",
    "    def eval(self, ground_examples, adversarials, labels, topk=1):\n",
    "        \"\"\" Evaluates how good the adversarial examples are\n",
    "        ARGS:\n",
    "            ground_truths: Variable (NxCxHxW) - examples before we did\n",
    "                           adversarial perturbation. Vals in [0, 1] range\n",
    "            adversarials: Variable (NxCxHxW) - examples after we did\n",
    "                           adversarial perturbation. Should be same shape and\n",
    "                           in same order as ground_truth\n",
    "            labels: Variable (longTensor N) - correct labels of classification\n",
    "                    output\n",
    "        RETURNS:\n",
    "            tuple of (% of correctly classified original examples,\n",
    "                      % of correctly classified adversarial examples)\n",
    "        \"\"\"\n",
    "        normed_ground = self.normalizer.forward(ground_examples)\n",
    "        ground_output = self.classifier_net.forward(normed_ground)\n",
    "        ground_ouput = ground_output[:, :, 0]\n",
    "        normed_advs = self.normalizer.forward(adversarials)\n",
    "        adv_output = self.classifier_net.forward(normed_advs)\n",
    "        adv_output = adv_output[:,:,0]\n",
    "\n",
    "        start_prec = utils.accuracy(ground_output.data, labels.data,\n",
    "                                    topk=(topk,))\n",
    "        adv_prec = utils.accuracy(adv_output.data, labels.data,\n",
    "                                  topk=(topk,))\n",
    "\n",
    "        return float(start_prec[0]), float(adv_prec[0])\n",
    "    \n",
    "    def eval_attack_only(self, adversarials, labels, topk=1):\n",
    "        \"\"\" Outputs the accuracy of the adv_inputs only\n",
    "        ARGS:\n",
    "            adv_inputs: Variable NxCxHxW - examples after we did adversarial\n",
    "                                           perturbation\n",
    "            labels: Variable (longtensor N) - correct labels of classification\n",
    "                                              output\n",
    "            topk: int - criterion for 'correct' classification\n",
    "        RETURNS:\n",
    "            (int) number of correctly classified examples\n",
    "        \"\"\"\n",
    "\n",
    "        normed_advs = self.normalizer.forward(adversarials)\n",
    "\n",
    "        adv_output = self.classifier_net.forward(normed_advs)\n",
    "        return utils.accuracy_int(adv_output, labels, topk=topk)\n",
    "\n",
    "\n",
    "\n",
    "    def print_eval_str(self, ground_examples, adversarials, labels, topk=1):\n",
    "        \"\"\" Prints how good this adversarial attack is\n",
    "            (explicitly prints out %CorrectlyClassified(ground_examples)\n",
    "            vs %CorrectlyClassified(adversarials)\n",
    "        ARGS:\n",
    "            ground_truths: Variable (NxCxHxW) - examples before we did\n",
    "                           adversarial perturbation. Vals in [0, 1] range\n",
    "            adversarials: Variable (NxCxHxW) - examples after we did\n",
    "                           adversarial perturbation. Should be same shape and\n",
    "                           in same order as ground_truth\n",
    "            labels: Variable (longTensor N) - correct labels of classification\n",
    "                    output\n",
    "        RETURNS:\n",
    "            None, prints some stuff though\n",
    "        \"\"\"\n",
    "\n",
    "        og, adv = self.eval(ground_examples, adversarials, labels, topk=topk)\n",
    "        print(\"Went from %s correct to %s correct\" % (og, adv))\n",
    "\n",
    "\n",
    "\n",
    "    def validation_loop(self, examples, labels, iter_no=None):\n",
    "        \"\"\" Prints out validation values interim for use in iterative techniques\n",
    "        ARGS:\n",
    "            new_examples: Variable (NxCxHxW) - [0.0, 1.0] images to be\n",
    "                          classified and compared against labels\n",
    "            labels: Variable (longTensor\n",
    "            N) - correct labels for indices of\n",
    "                             examples\n",
    "            iter_no: String - an extra thing for prettier prints\n",
    "        RETURNS:\n",
    "            None\n",
    "        \"\"\"\n",
    "        normed_input = self.normalizer.forward(examples)\n",
    "        new_output = self.classifier_net.forward(normed_input)\n",
    "        \n",
    "        \n",
    "        new_prec = utils.accuracy(new_output.data, labels.data, topk=(1,))\n",
    "        print_str = \"\"\n",
    "        if isinstance(iter_no, int):\n",
    "            if iter_no %10 == 0:\n",
    "                print_str += \"(iteration %02d): \" % iter_no\n",
    "#         elif isinstance(iter_no, string_types):\n",
    "#             print_str += \"(%s): \" % iter_no\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        print_str += \" %s correct\" % float(new_prec[0])\n",
    "\n",
    "        print(print_str)\n",
    "\n",
    "class PGD(AdversarialAttack):\n",
    "\n",
    "    def __init__(self, classifier_net, normalizer, threat_model, loss_fxn,\n",
    "                 manual_gpu=None):\n",
    "        super(PGD, self).__init__(classifier_net, normalizer, threat_model,\n",
    "                                  manual_gpu=manual_gpu)\n",
    "        self.loss_fxn = loss_fxn # WE MAXIMIZE THIS!!!\n",
    "\n",
    "    def attack(self, examples, labels, step_size=1.0/255.0,\n",
    "               num_iterations=20, random_init=False, signed=True,\n",
    "               optimizer=None, optimizer_kwargs=None,\n",
    "               loss_convergence=0.999, verbose=True,\n",
    "               keep_best=True):\n",
    "        \"\"\" Builds PGD examples for the given examples with l_inf bound and\n",
    "            given step size. Is almost identical to the BIM attack, except\n",
    "            we take steps that are proportional to gradient value instead of\n",
    "            just their sign.\n",
    "        ARGS:\n",
    "            examples: NxCxHxW tensor - for N examples, is NOT NORMALIZED\n",
    "                      (i.e., all values are in between 0.0 and 1.0)\n",
    "            labels: N longTensor - single dimension tensor with labels of\n",
    "                    examples (in same order as examples)\n",
    "            l_inf_bound : float - how much we're allowed to perturb each pixel\n",
    "                          (relative to the 0.0, 1.0 range)\n",
    "            step_size : float - how much of a step we take each iteration\n",
    "            num_iterations: int or pair of ints - how many iterations we take.\n",
    "                            If pair of ints, is of form (lo, hi), where we run\n",
    "                            at least 'lo' iterations, at most 'hi' iterations\n",
    "                            and we quit early if loss has stabilized.\n",
    "            random_init : bool - if True, we randomly pick a point in the\n",
    "                               l-inf epsilon ball around each example\n",
    "            signed : bool - if True, each step is\n",
    "                            adversarial = adversarial + sign(grad)\n",
    "                            [this is the form that madry et al use]\n",
    "                            if False, each step is\n",
    "                            adversarial = adversarial + grad\n",
    "            keep_best : bool - if True, we keep track of the best adversarial\n",
    "                               perturbations per example (in terms of maximal\n",
    "                               loss) in the minibatch. The output is the best of\n",
    "                               each of these then\n",
    "        RETURNS:\n",
    "            AdversarialPerturbation object with correct parameters.\n",
    "            Calling perturbation() gets Variable of output and\n",
    "            calling perturbation().data gets tensor of output\n",
    "        \"\"\"\n",
    "\n",
    "        ######################################################################\n",
    "        #   Setups and assertions                                            #\n",
    "        ######################################################################\n",
    "\n",
    "        self.classifier_net.eval()\n",
    "\n",
    "        if not verbose:\n",
    "            self.validator = lambda ex, label, iter_no: None\n",
    "        else:\n",
    "            self.validator = self.validation_loop\n",
    "\n",
    "        perturbation = self.threat_model(examples)\n",
    "\n",
    "        num_examples = examples.shape[0]\n",
    "        var_examples = Variable(examples, requires_grad=True)\n",
    "        var_labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "        if isinstance(num_iterations, int):\n",
    "            min_iterations = num_iterations\n",
    "            max_iterations = num_iterations\n",
    "        elif isinstance(num_iterations, tuple):\n",
    "            min_iterations, max_iterations = num_iterations\n",
    "\n",
    "        best_perturbation = None\n",
    "        if keep_best:\n",
    "            best_loss_per_example = {i: None for i in range(num_examples)}\n",
    "\n",
    "        prev_loss = None\n",
    "\n",
    "        ######################################################################\n",
    "        #   Loop through iterations                                          #\n",
    "        ######################################################################\n",
    "\n",
    "        self.loss_fxn.setup_attack_batch(var_examples)\n",
    "        self.validator(var_examples, var_labels, iter_no=\"START\")\n",
    "\n",
    "        # random initialization if necessary\n",
    "        if random_init:\n",
    "            perturbation.random_init()\n",
    "            self.validator(perturbation(var_examples), var_labels,\n",
    "                           iter_no=\"RANDOM\")\n",
    "\n",
    "        # Build optimizer techniques for both signed and unsigned methods\n",
    "        optimizer = optimizer or optim.Adam\n",
    "        if optimizer_kwargs is None:\n",
    "            optimizer_kwargs = {'lr':0.0001}\n",
    "        optimizer = optimizer(perturbation.parameters(), **optimizer_kwargs)\n",
    "\n",
    "        update_fxn = lambda grad_data: -1 * step_size * torch.sign(grad_data)\n",
    "\n",
    "\n",
    "        for iter_no in range(max_iterations):\n",
    "            perturbation.zero_grad()\n",
    "            loss = self.loss_fxn.forward(perturbation(var_examples), var_labels,\n",
    "                                         perturbation=perturbation,\n",
    "                                         output_per_example=keep_best)\n",
    "            loss_per_example = loss\n",
    "            loss = loss.sum()\n",
    "\n",
    "            loss = -1 * loss\n",
    "            torch.autograd.backward(loss.sum())\n",
    "\n",
    "            if signed:\n",
    "                perturbation.update_params(update_fxn)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            if keep_best:\n",
    "                mask_val = torch.zeros(num_examples, dtype=torch.uint8)\n",
    "                for i, el in enumerate(loss_per_example):\n",
    "                    this_best_loss = best_loss_per_example[i]\n",
    "                    if this_best_loss is None or this_best_loss[1] < float(el):\n",
    "                        mask_val[i] = 1\n",
    "                        best_loss_per_example[i] = (iter_no, float(el))\n",
    "\n",
    "                if best_perturbation is None:\n",
    "                    best_perturbation = self.threat_model(examples)\n",
    "\n",
    "                best_perturbation = perturbation.merge_perturbation(\n",
    "                                                            best_perturbation,\n",
    "                                                            mask_val)\n",
    "\n",
    "\n",
    "            self.validator((best_perturbation or perturbation)(var_examples),\n",
    "                           var_labels, iter_no=iter_no)\n",
    "\n",
    "            # Stop early if loss didn't go down too much\n",
    "            if (iter_no >= min_iterations and\n",
    "                float(loss) >= loss_convergence * prev_loss):\n",
    "                if verbose:\n",
    "                    print(\"Stopping early at %03d iterations\" % iter_no)\n",
    "                break\n",
    "            prev_loss = float(loss)\n",
    "\n",
    "\n",
    "\n",
    "        perturbation.zero_grad()\n",
    "        self.loss_fxn.cleanup_attack_batch()\n",
    "        perturbation.attach_originals(examples)\n",
    "        return perturbation\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from numbers import Number\n",
    "# from .utils import pytorch_utils as utils\n",
    "# from .utils import image_utils as img_utils\n",
    "# from . import spatial_transformers as st\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "# from . import adversarial_perturbations as ap\n",
    "\n",
    "\"\"\" Loss function building blocks \"\"\"\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                        LOSS FUNCTION WRAPPER                               #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "class RegularizedLoss(object):\n",
    "    \"\"\" Wrapper for multiple PartialLoss objects where we combine with\n",
    "        regularization constants \"\"\"\n",
    "    def __init__(self, losses, scalars, negate=False):\n",
    "        \"\"\"\n",
    "        ARGS:\n",
    "            losses : dict - dictionary of partialLoss objects, each is keyed\n",
    "                            with a nice identifying name\n",
    "            scalars : dict - dictionary of scalars, each is keyed with the\n",
    "                             same identifying name as is in self.losses\n",
    "            negate : bool - if True, we negate the whole thing at the end\n",
    "        \"\"\"\n",
    "\n",
    "        assert sorted(losses.keys()) == sorted(scalars.keys())\n",
    "\n",
    "        self.losses = losses\n",
    "        self.scalars = scalars\n",
    "        self.negate = negate\n",
    "\n",
    "    def forward(self, examples, labels, *args, **kwargs):\n",
    "\n",
    "        output = None\n",
    "        output_per_example = kwargs.get('output_per_example', False)\n",
    "        for k in self.losses:\n",
    "            loss = self.losses[k]\n",
    "            scalar = self.scalars[k]\n",
    "            loss_val = loss.forward(examples, labels, *args, **kwargs)\n",
    "            # assert scalar is either a...\n",
    "            assert (isinstance(scalar, float) or # number\n",
    "                    scalar.numel() == 1 or # tf wrapping of a number\n",
    "                    scalar.shape == loss_val.shape) # same as the loss_val\n",
    "\n",
    "            addendum = loss_val * scalar\n",
    "            if addendum.numel() > 1:\n",
    "                if not output_per_example:\n",
    "                    addendum = torch.sum(addendum)\n",
    "\n",
    "            if output is None:\n",
    "                output = addendum\n",
    "            else:\n",
    "                output = output + addendum\n",
    "        if self.negate:\n",
    "            return output * -1\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def setup_attack_batch(self, fix_im):\n",
    "        \"\"\" Setup before calling loss on a new minibatch. Ensures the correct\n",
    "            fix_im for reference regularizers and that all grads are zeroed\n",
    "        ARGS:\n",
    "            fix_im: Variable (NxCxHxW) - Ground images for this minibatch\n",
    "                    SHOULD BE IN [0.0, 1.0] RANGE\n",
    "        \"\"\"\n",
    "        for loss in self.losses.values():\n",
    "            if isinstance(loss, ReferenceRegularizer):\n",
    "                loss.setup_attack_batch(fix_im)\n",
    "            else:\n",
    "                loss.zero_grad()\n",
    "\n",
    "\n",
    "    def cleanup_attack_batch(self):\n",
    "        \"\"\" Does some cleanup stuff after we finish on a minibatch:\n",
    "        - clears the fixed images for ReferenceRegularizers\n",
    "        - zeros grads\n",
    "        - clears example-based scalars (i.e. scalars that depend on which\n",
    "          example we're using)\n",
    "        \"\"\"\n",
    "        for loss in self.losses.values():\n",
    "            if isinstance(loss, ReferenceRegularizer):\n",
    "                loss.cleanup_attack_batch()\n",
    "            else:\n",
    "                loss.zero_grad()\n",
    "\n",
    "        for key, scalar in self.scalars.items():\n",
    "            if not isinstance(scalar, Number):\n",
    "                self.scalars[key] = None\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for loss in self.losses.values():\n",
    "            loss.zero_grad() # probably zeros the same net more than once...\n",
    "\n",
    "\n",
    "\n",
    "class PartialLoss(object):\n",
    "    \"\"\" Partially applied loss object. Has forward and zero_grad methods \"\"\"\n",
    "    def __init__(self):\n",
    "        self.nets = []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for net in self.nets:\n",
    "            net.zero_grad()\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                                  LOSS FUNCTIONS                            #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "############################################################################\n",
    "#                       NAIVE CORRECT INDICATOR LOSS                       #\n",
    "############################################################################\n",
    "\n",
    "class IncorrectIndicator(PartialLoss):\n",
    "    def __init__(self, classifier, normalizer=None):\n",
    "        super(IncorrectIndicator, self).__init__()\n",
    "        self.classifier = classifier\n",
    "        self.normalizer = normalizer\n",
    "\n",
    "    def forward(self, examples, labels, *args, **kwargs):\n",
    "        \"\"\" Returns either (the number | a boolean vector) of examples that\n",
    "            don't match the labels when run through the\n",
    "            classifier(normalizer(.)) composition.\n",
    "        ARGS:\n",
    "            examples: Variable (NxCxHxW) - should be same shape as\n",
    "                      ctx.fix_im, is the examples we define loss for.\n",
    "                      SHOULD BE IN [0.0, 1.0] RANGE\n",
    "            labels: Variable (longTensor of length N) - true classification\n",
    "                    output for fix_im/examples\n",
    "        KWARGS:\n",
    "            return_type: String - either 'int' or 'vector'. If 'int', we return\n",
    "                         the number of correctly classified examples,\n",
    "                         if 'vector' we return a boolean length-N longtensor\n",
    "                         with the indices of\n",
    "        RETURNS:\n",
    "            scalar loss variable or boolean vector, depending on kwargs\n",
    "        \"\"\"\n",
    "        return_type = kwargs.get('return_type', 'int')\n",
    "        assert return_type in ['int', 'vector']\n",
    "\n",
    "        class_out = self.classifier.forward(self.normalizer.forward(examples))\n",
    "\n",
    "        _, outputs = torch.max(class_out, 1)\n",
    "        incorrect_indicator = outputs != labels\n",
    "\n",
    "        if return_type == 'int':\n",
    "            return torch.sum(incorrect_indicator)\n",
    "        else:\n",
    "            return incorrect_indicator\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                   Standard XEntropy Loss                   #\n",
    "##############################################################################\n",
    "\n",
    "class PartialXentropy(PartialLoss):\n",
    "    def __init__(self, classifier, normalizer=None):\n",
    "        super(PartialXentropy, self).__init__()\n",
    "        self.classifier = classifier\n",
    "        self.normalizer = normalizer\n",
    "        self.nets.append(self.classifier)\n",
    "\n",
    "    def forward(self, examples, labels, *args, **kwargs):\n",
    "        \"\"\" Returns XEntropy loss\n",
    "        ARGS:\n",
    "            examples: Variable (NxCxHxW) - should be same shape as\n",
    "                      ctx.fix_im, is the examples we define loss for.\n",
    "                      SHOULD BE IN [0.0, 1.0] RANGE\n",
    "            labels: Variable (longTensor of length N) - true classification\n",
    "                    output for fix_im/examples\n",
    "        RETURNS:\n",
    "            scalar loss variable\n",
    "        \"\"\"\n",
    "\n",
    "        if self.normalizer is not None:\n",
    "            normed_examples = self.normalizer.forward(examples)\n",
    "        else:\n",
    "            normed_examples = examples\n",
    "\n",
    "        xentropy_init_kwargs = {}\n",
    "        if kwargs.get('output_per_example') == True:\n",
    "            xentropy_init_kwargs['reduction'] = 'none'\n",
    "        criterion = nn.CrossEntropyLoss(**xentropy_init_kwargs)\n",
    "        return criterion(self.classifier.forward(normed_examples), labels)\n",
    "\n",
    "##############################################################################\n",
    "#                           Carlini Wagner loss functions                    #\n",
    "##############################################################################\n",
    "\n",
    "class CWLossF6(PartialLoss):\n",
    "    def __init__(self, classifier, normalizer=None, kappa=0.0):\n",
    "        super(CWLossF6, self).__init__()\n",
    "        self.classifier = classifier\n",
    "#         self.normalizer = normalizer\n",
    "        self.normalizer = normalizer or utils.IdentityNormalize()\n",
    "        self.nets.append(self.classifier)\n",
    "        self.kappa = kappa\n",
    "\n",
    "\n",
    "    def forward(self, examples, labels, *args, **kwargs):\n",
    "        classifier_in = self.normalizer.forward(examples)\n",
    "        classifier_out = self.classifier.forward(classifier_in)\n",
    "        \n",
    "#         print(new_output.shape)\n",
    "        \n",
    "        # get target logits\n",
    "        target_logits = torch.gather(classifier_out, 1, labels.view(-1, 1))\n",
    "\n",
    "        # get largest non-target logits\n",
    "        max_2_logits, argmax_2_logits = torch.topk(classifier_out, 2, dim=1)\n",
    "        top_max, second_max = max_2_logits.chunk(2, dim=1)\n",
    "        top_argmax, _ = argmax_2_logits.chunk(2, dim=1)\n",
    "        targets_eq_max = top_argmax.squeeze().eq(labels).float().view(-1, 1)\n",
    "        targets_ne_max = top_argmax.squeeze().ne(labels).float().view(-1, 1)\n",
    "        max_other = targets_eq_max * second_max + targets_ne_max * top_max\n",
    "\n",
    "\n",
    "        if kwargs.get('targeted', False):\n",
    "            # in targeted case, want to make target most likely\n",
    "            f6 = torch.clamp(max_other - target_logits, min=-1 * self.kappa)\n",
    "        else:\n",
    "            # in NONtargeted case, want to make NONtarget most likely\n",
    "            f6 = torch.clamp(target_logits - max_other, min=-1 * self.kappa)\n",
    "\n",
    "        return f6.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                               REFERENCE REGULARIZERS                       #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\"\"\" Regularization terms that refer back to a set of 'fixed images', or the\n",
    "    original images.\n",
    "    example: L2 regularization which computes L2dist between a perturbed image\n",
    "             and the FIXED ORIGINAL IMAGE\n",
    "    NOTE: it's important that these return Variables that are scalars\n",
    "    (output.numel() == 1), otherwise there's a memory leak w/ CUDA.\n",
    "    See my discussion on this here:\n",
    "        https://discuss.pytorch.org/t/cuda-memory-not-being-freed/15965\n",
    "\"\"\"\n",
    "\n",
    "class ReferenceRegularizer(PartialLoss):\n",
    "    def __init__(self, fix_im):\n",
    "        super(ReferenceRegularizer, self).__init__()\n",
    "        self.fix_im = fix_im\n",
    "\n",
    "    def setup_attack_batch(self, fix_im):\n",
    "        \"\"\" Setup function to ensure fixed images are set\n",
    "            has been made; also zeros grads\n",
    "        ARGS:\n",
    "            fix_im: Variable (NxCxHxW) - Ground images for this minibatch\n",
    "                    SHOULD BE IN [0.0, 1.0] RANGE\n",
    "        \"\"\"\n",
    "        self.fix_im = fix_im\n",
    "        self.zero_grad()\n",
    "\n",
    "\n",
    "    def cleanup_attack_batch(self):\n",
    "        \"\"\" Cleanup function to clear the fixed images after an attack batch\n",
    "            has been made; also zeros grads\n",
    "        \"\"\"\n",
    "        old_fix_im = self.fix_im\n",
    "        self.fix_im = None\n",
    "        del old_fix_im\n",
    "        self.zero_grad()\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                               SOFT L_INF REGULARIZATION                   #\n",
    "#############################################################################\n",
    "\n",
    "class SoftLInfRegularization(ReferenceRegularizer):\n",
    "    '''\n",
    "        see page 10 of this paper (https://arxiv.org/pdf/1608.04644.pdf)\n",
    "        for discussion on why we want SOFT l inf\n",
    "    '''\n",
    "    def __init__(self, fix_im, **kwargs):\n",
    "        super(SoftLInfRegularization, self).__init__(fix_im)\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        # ARGS should have one element, which serves as the tau value\n",
    "\n",
    "        tau =  8.0 / 255.0  # starts at 1 each time?\n",
    "        scale_factor = 0.9\n",
    "        l_inf_dist = float(torch.max(torch.abs(examples - self.fix_im)))\n",
    "        '''\n",
    "        while scale_factor * tau > l_inf_dist:\n",
    "            tau *= scale_factor\n",
    "        assert tau > l_inf_dist\n",
    "        '''\n",
    "        delta_minus_taus = torch.clamp(torch.abs(examples - self.fix_im) - tau,\n",
    "                                       min=0.0)\n",
    "        batchwise = utils.batchwise_norm(delta_minus_taus, 'inf', dim=0)\n",
    "        return batchwise.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                               L2 REGULARIZATION                           #\n",
    "#############################################################################\n",
    "\n",
    "class L2Regularization(ReferenceRegularizer):\n",
    "\n",
    "    def __init__(self, fix_im, **kwargs):\n",
    "        super(L2Regularization, self).__init__(fix_im)\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        l2_dist = img_utils.nchw_l2(examples, self.fix_im,\n",
    "                                    squared=True).view(-1, 1)\n",
    "        return l2_dist.squeeze()\n",
    "\n",
    "#############################################################################\n",
    "#                         LPIPS PERCEPTUAL REGULARIZATION                   #\n",
    "#############################################################################\n",
    "\n",
    "class LpipsRegularization(ReferenceRegularizer):\n",
    "\n",
    "    def __init__(self, fix_im, **kwargs):\n",
    "        super(LpipsRegularization, self).__init__(fix_im)\n",
    "\n",
    "        manual_gpu = kwargs.get('manual_gpu', None)\n",
    "        if manual_gpu is not None:\n",
    "            self.use_gpu = manual_gpu\n",
    "        else:\n",
    "            self.use_gpu = utils.use_gpu()\n",
    "\n",
    "        self.dist_model = dm.DistModel(net='alex', manual_gpu=self.use_gpu)\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        xform = lambda im: im * 2.0 - 1.0\n",
    "        perceptual_loss = self.dist_model.forward_var(examples,\n",
    "                                                      self.fix_im)\n",
    "\n",
    "        return perceptual_loss.squeeze()\n",
    "\n",
    "#############################################################################\n",
    "#                         SSIM PERCEPTUAL REGULARIZATION                    #\n",
    "#############################################################################\n",
    "\n",
    "class SSIMRegularization(ReferenceRegularizer):\n",
    "\n",
    "    def __init__(self, fix_im, **kwargs):\n",
    "        super(SSIMRegularization, self).__init__(fix_im)\n",
    "\n",
    "        if 'window_size' in kwargs:\n",
    "            self.ssim_instance = ssim.SSIM(window_size=kwargs['window_size'])\n",
    "        else:\n",
    "            self.ssim_instance = ssim.SSIM()\n",
    "\n",
    "        manual_gpu = kwargs.get('manual_gpu', None)\n",
    "        if manual_gpu is not None:\n",
    "            self.use_gpu = manual_gpu\n",
    "        else:\n",
    "            self.use_gpu = utils.use_gpu()\n",
    "\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        output = []\n",
    "        for ex, fix_ex in zip(examples, self.fix_im):\n",
    "            output.append(1.0 - self.ssim_instance(ex.unsqueeze(0),\n",
    "                                                   fix_ex.unsqueeze(0)))\n",
    "        return torch.stack(output)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                           SPATIAL LOSS FUNCTIONS                           #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "class FullSpatialLpLoss(PartialLoss):\n",
    "    \"\"\" Spatial loss using lp norms on the spatial transformation parameters\n",
    "    This is defined as the Lp difference between the identity map and the\n",
    "    provided spatial transformation parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FullSpatialLpLoss, self).__init__()\n",
    "\n",
    "        lp = kwargs.get('lp', 2)\n",
    "        assert lp in [1, 2, 'inf']\n",
    "        self.lp = lp\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        \"\"\" Computes lp loss between identity map and spatial transformation.\n",
    "            There better be a kwarg with key 'spatial' which is as FullSpatial\n",
    "            object describing how the examples were generated from the originals\n",
    "        \"\"\"\n",
    "        st_obj = kwargs['spatial']\n",
    "        assert isinstance(st_obj, st.FullSpatial)\n",
    "\n",
    "\n",
    "        # First create the identity map and make same type as examples\n",
    "        identity_map = Variable(st_obj.identity_params(examples.shape))\n",
    "        if examples.is_cuda:\n",
    "            identity_map.cuda()\n",
    "\n",
    "        # Then take diffs and take lp norms\n",
    "        diffs = st_obj.grid_params - identity_map\n",
    "        lp_norm = utils.batchwise_norm(diffs, self.lp, dim=0)\n",
    "        return lp_norm # return Nx1 variable, will sum in parent class\n",
    "\n",
    "\n",
    "class PerturbationNormLoss(PartialLoss):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PerturbationNormLoss, self).__init__()\n",
    "\n",
    "        lp = kwargs.get('lp', 2)\n",
    "        assert lp in [1, 2, 'inf']\n",
    "        self.lp = lp\n",
    "\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        \"\"\" Computes perturbation norm and multiplies by scale\n",
    "        There better be a kwarg with key 'perturbation' which is a perturbation\n",
    "        object with a 'perturbation_norm' method that takes 'lp_style' as a\n",
    "        kwarg\n",
    "        \"\"\"\n",
    "\n",
    "        perturbation = kwargs['perturbation']\n",
    "        assert isinstance(perturbation, AdversarialPerturbation)\n",
    "\n",
    "        return perturbation.perturbation_norm(lp_style=self.lp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                       Combined Transformer Loss                            #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "class CombinedTransformerLoss(ReferenceRegularizer):\n",
    "    \"\"\" General class for distance functions and loss functions of the form\n",
    "    min_T ||X - T(Y)|| + c * || T ||\n",
    "    where X is the original image, and Y is the 'adversarial' input image.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, fix_im, transform_class=None,\n",
    "                 regularization_constant=1.0,\n",
    "                 transformation_loss=partial(utils.summed_lp_norm,lp=2),\n",
    "                 transform_norm_kwargs=None):\n",
    "        \"\"\" Takes in a reference fix im and a class of transformations we need\n",
    "            to search over to compute forward.\n",
    "        \"\"\"\n",
    "        super(CombinedTransformerLoss, self).__init__(fix_im)\n",
    "        self.transform_class = transform_class\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.transformation_loss = transformation_loss\n",
    "        self.transform_norm_kwargs = transform_norm_kwargs or {}\n",
    "        self.transformer = None\n",
    "\n",
    "\n",
    "    def cleanup_attack_batch(self):\n",
    "        super(CombinedTransformerLoss, self).cleanup_attack_batch()\n",
    "        self.transformer = None\n",
    "\n",
    "\n",
    "    def _inner_loss(self, examples):\n",
    "        \"\"\" Computes the combined loss for a particular transformation \"\"\"\n",
    "\n",
    "        trans_examples = self.transformer.forward(examples)\n",
    "        trans_loss = self.transformation_loss(self.fix_im - trans_examples)\n",
    "\n",
    "        trans_norm = self.transformer.norm(**self.transform_norm_kwargs)\n",
    "        return trans_loss + trans_norm * self.regularization_constant\n",
    "\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        \"\"\" Computes the distance between examples and args\n",
    "        ARGS:\n",
    "            examples : NxCxHxW Variable - 'adversarially' perturbed image from\n",
    "                       the self.fix_im\n",
    "        KWARGS:\n",
    "            optimization stuff here\n",
    "        \"\"\"\n",
    "\n",
    "        ######################################################################\n",
    "        #   Setup transformer + optimizer                                    #\n",
    "        ######################################################################\n",
    "        self.transformer = self.transform_class(shape=examples.shape)\n",
    "\n",
    "\n",
    "        optim_kwargs = kwargs.get('xform_loss_optim_kwargs', {})\n",
    "        optim_type = kwargs.get('xform_loss_optim_type', torch.optim.Adam)\n",
    "        num_iter = kwargs.get('xform_loss_num_iter', 20)\n",
    "\n",
    "        optimizer = optim_type(self.transformer.parameters(), **optim_kwargs)\n",
    "\n",
    "        #####################################################################\n",
    "        #   Iterate and optimize the transformer                            #\n",
    "        #####################################################################\n",
    "        for iter_no in range(num_iter):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self._inner_loss(examples)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return self._inner_loss(examples)\n",
    "\n",
    "\n",
    "\n",
    "class RelaxedTransformerLoss(ReferenceRegularizer):\n",
    "    \"\"\"  Relaxed version of transformer loss: assumes that the adversarial\n",
    "         examples are of the form Y=S(X) + delta for some S in the\n",
    "         transformation class and some small delta perturbation outside the\n",
    "         perturbation.\n",
    "         In this case, we just compute ||delta|| + c||S||\n",
    "         This saves us from having to do the inner minmization step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fix_im,\n",
    "                 regularization_constant=1.0,\n",
    "                 transformation_loss=partial(utils.summed_lp_norm,lp=2),\n",
    "                 transform_norm_kwargs=None):\n",
    "        \"\"\" Takes in a reference fix im and a class of transformations we need\n",
    "            to search over to compute forward.\n",
    "        \"\"\"\n",
    "        super(RelaxedTransformerLoss, self).__init__(fix_im)\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.transformation_loss = transformation_loss\n",
    "        self.transform_norm_kwargs = transform_norm_kwargs or {}\n",
    "\n",
    "\n",
    "    def forward(self, examples, *args, **kwargs):\n",
    "        \"\"\" Computes the distance between examples and args\n",
    "        ARGS:\n",
    "            examples : NxCxHxW Variable - 'adversarially' perturbed image from\n",
    "                       the self.fix_im\n",
    "        KWARGS:\n",
    "            optimization stuff here\n",
    "        \"\"\"\n",
    "\n",
    "        # Collect transformer norm\n",
    "        transformer = kwargs['transformer']\n",
    "        assert isinstance(transformer, st.ParameterizedTransformation)\n",
    "\n",
    "        transformer_norm = self.regularization_constant *\\\n",
    "                           transformer.norm(**self.transform_norm_kwargs)\n",
    "\n",
    "        # Collect transformation loss\n",
    "        delta = self.transformer.forward(self.fix_im) - examples\n",
    "        transformation_loss = self.transformation_loss(delta)\n",
    "\n",
    "        return transformation_loss + transformer_norm\n",
    "    \n",
    "\"\"\" File that holds adversarial perturbations as torch.nn.Modules.\n",
    "    An adversarial perturbation is an example-specific\n",
    "TODO: this needs to be fleshed out, as a general technique to make adversarial\n",
    "      perturbations.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from . import spatial_transformers as st\n",
    "# from .utils import image_utils as img_utils\n",
    "# from .utils import pytorch_utils as utils\n",
    "from torch.autograd import Variable\n",
    "import functools\n",
    "\n",
    "\n",
    "# assert initialized decorator\n",
    "def initialized(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        assert self.initialized, (\"Parameters not initialized yet. \"\n",
    "                                  \"Call .forward(...) first\")\n",
    "        return func(self, *args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                                   SKELETON CLASS                           #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "class AdversarialPerturbation(nn.Module):\n",
    "    \"\"\" Skeleton class to hold adversarial perturbations FOR A SINGLE MINIBATCH.\n",
    "        For general input-agnostic adversarial perturbations, see the\n",
    "        ThreatModel class\n",
    "        All subclasses need the following:\n",
    "        - perturbation_norm() : no args -> scalar Variable\n",
    "        - self.parameters() needs to iterate over params we want to optimize\n",
    "        - constrain_params() : no args -> no return,\n",
    "             modifies the parameters such that this is still a valid image\n",
    "        - forward : no args -> Variable - applies the adversarial perturbation\n",
    "                    the originals and outputs a Variable of how we got there\n",
    "        - adversarial_tensors() : applies the adversarial transform to the\n",
    "                                  originals and outputs TENSORS that are the\n",
    "                                  adversarial images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threat_model, perturbation_params):\n",
    "\n",
    "        super(AdversarialPerturbation, self).__init__()\n",
    "        self.threat_model = threat_model\n",
    "        self.initialized = False\n",
    "        self.perturbation_params = perturbation_params\n",
    "\n",
    "        if isinstance(perturbation_params, tuple):\n",
    "            self.use_gpu = perturbation_params[1].use_gpu or utils.use_gpu()\n",
    "        else:\n",
    "            self.use_gpu = perturbation_params.use_gpu or utils.use_gpu()\n",
    "        # Stores parameters of the adversarial perturbation and hyperparams\n",
    "        # to compute total perturbation norm here\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if isinstance(self.perturbation_params, tuple):\n",
    "            output_str = \"[Perturbation] %s: %s\" % (self.__class__.__name__,\n",
    "                                                    self.perturbation_params[1])\n",
    "            output_str += '\\n['\n",
    "            for el in self.perturbation_params[0]:\n",
    "                output_str += '\\n\\t%s,' % el\n",
    "            output_str += '\\n]'\n",
    "            return output_str\n",
    "        else:\n",
    "            return \"[Perturbation] %s: %s\"  % (self.__class__.__name__,\n",
    "                                               self.perturbation_params)\n",
    "\n",
    "    def _merge_setup(self, *args):\n",
    "        \"\"\" Internal method to be used when initializing a new perturbation\n",
    "            from merging only. Should not be called outside this file!!\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, x):\n",
    "        \"\"\" This is the standard setup technique and should be used to\n",
    "            initialize a perturbation (i.e. sets up parameters and unlocks\n",
    "            all other methods)\n",
    "        ARGS:\n",
    "            x : Variable or Tensor (NxCxHxW) - the images this perturbation is\n",
    "                intended for\n",
    "        \"\"\"\n",
    "        self.num_examples = x.shape[0]\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def perturbation_norm(self, x=None):\n",
    "        \"\"\" This returns the 'norm' of this perturbation. Optionally, for\n",
    "            certain norms, having access to the images for which the\n",
    "            perturbation is intended can have an effect on the output.\n",
    "        ARGS:\n",
    "            x : Variable or Tensor (NxCxHxW) - optionally can be the images\n",
    "                that the perturbation was intended for\n",
    "        RETURNS:\n",
    "            Scalar Variable\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to call subclass method here\")\n",
    "\n",
    "    @initialized\n",
    "    def constrain_params(self):\n",
    "        \"\"\" This modifies the parameters such that the perturbation falls within\n",
    "            the threat model it belongs to. E.g. for l-infinity threat models,\n",
    "            this clips the params to match the right l-infinity bound.\n",
    "            TODO: for non-lp norms, projecting to the nearest point in the level\n",
    "                  set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to call subclass method here\")\n",
    "\n",
    "    @initialized\n",
    "    def make_valid_image(self, x):\n",
    "        \"\"\" This takes in the minibatch self's parameters were tuned for and\n",
    "            clips the parameters such that this is still a valid image.\n",
    "        ARGS:\n",
    "            x : Variable or Tensor (NxCxHxW) - the images this this perturbation\n",
    "                was intended for\n",
    "        RETURNS:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass # Only implement in classes that can create invalid images\n",
    "\n",
    "    @initialized\n",
    "    def forward(self, x):\n",
    "        \"\"\" This takes in the minibatch self's parameters were tuned for and\n",
    "            outputs a variable of the perturbation applied to the images\n",
    "        ARGS:\n",
    "            x : Variable (NxCxHxW) - the images this this perturbation\n",
    "                was intended for\n",
    "        RETURNS:\n",
    "            Variable (NxCxHxW) - the perturbation applied to the input images\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to call subclass method here\")\n",
    "\n",
    "    @initialized\n",
    "    def add_to_params(self, grad_data):\n",
    "        \"\"\" This takes in a Tensor the same shape as self's parameters and\n",
    "            adds to them. Note that this usually won't preserve gradient\n",
    "            information\n",
    "            (also this might have different signatures in subclasses)\n",
    "        ARGS:\n",
    "            x : Tensor (params-shape) - Tensor to be added to the\n",
    "                parameters of self\n",
    "        RETURNS:\n",
    "            None, but modifies self's parameters\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to call subclass method here\")\n",
    "\n",
    "    @initialized\n",
    "    def update_params(self, step_fxn):\n",
    "        \"\"\" This takes in a function step_fxn: Tensor -> Tensor that generates\n",
    "            the change to the parameters that we step along. This loops through\n",
    "            all parameters and updates signs accordingly.\n",
    "            For sequential perturbations, this also multiplies by a scalar if\n",
    "            provided\n",
    "        ARGS:\n",
    "            step_fxn : Tensor -> Tensor - function that maps tensors to tensors.\n",
    "                       e.g. for FGSM, we want a function that multiplies signs\n",
    "                       by step_size\n",
    "        RETURNS:\n",
    "            None, but updates the parameters\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to call subclass method here\")\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def adversarial_tensors(self, x=None):\n",
    "        \"\"\" Little helper method to get the tensors of the adversarial images\n",
    "            directly\n",
    "        \"\"\"\n",
    "        assert x is not None or self.originals is not None\n",
    "        if x is None:\n",
    "            x = self.originals\n",
    "\n",
    "        return self.forward(x).data\n",
    "\n",
    "    @initialized\n",
    "    def attach_attr(self, attr_name, attr):\n",
    "        \"\"\" Special method to set an attribute if it doesn't exist in this\n",
    "            object yet. throws error if this attr already exists\n",
    "        ARGS:\n",
    "            attr_name : string - name of attribute we're attaching\n",
    "            attr: object - attribute we're attaching\n",
    "        RETURNS:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if hasattr(self, attr_name):\n",
    "            raise Exception(\"%s already has attribute %s\" % (self, attr_name))\n",
    "        else:\n",
    "            setattr(self, attr_name, attr)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def attach_originals(self, originals):\n",
    "        \"\"\" Little helper method to tack on the original images to self to\n",
    "            pass around the (images, perturbation) in a single object\n",
    "        \"\"\"\n",
    "        self.attach_attr('originals', originals)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def random_init(self):\n",
    "        \"\"\" Modifies the parameters such that they're randomly initialized\n",
    "            uniformly across the threat model (this is harder for nonLp threat\n",
    "            models...). Takes no args and returns nothing, but modifies the\n",
    "            parameters\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to call subclass method here\")\n",
    "\n",
    "    @initialized\n",
    "    def merge_perturbation(self, other, self_mask):\n",
    "        \"\"\" Special technique to merge this perturbation with another\n",
    "            perturbation of the same threat model.\n",
    "            This will return a new perturbation object that, for each parameter\n",
    "            will return the parameters of self for self_mask, and the\n",
    "            perturbation of other for NOT(self_mask)\n",
    "        ARGS:\n",
    "            other: AdversarialPerturbation Object - instance of other\n",
    "                   adversarial perturbation that is instantiated with the\n",
    "                   same threat model as self\n",
    "            self_indices: ByteTensor [N] : bytetensor indicating which\n",
    "                          parameters to include from self and which to include\n",
    "                          from other\n",
    "        \"\"\"\n",
    "\n",
    "        # this parent class just does the shared asserts such that this is a\n",
    "        # valid thing\n",
    "        assert self.__class__ == other.__class__\n",
    "        assert self.threat_model == other.threat_model\n",
    "        assert self.num_examples == other.num_examples\n",
    "        assert self.perturbation_params == other.perturbation_params\n",
    "        assert other.initialized\n",
    "\n",
    "#         this function is new\n",
    "    @initialized\n",
    "    def collect_all(self, classifier_net, normalizer):\n",
    "        \"\"\" Returns a list of [adversarials, originals] of the SUCCESSFUL\n",
    "            attacks only, according to the given classifier_net, normalizer\n",
    "            SUCCESSFUL here means that the adversarial is different\n",
    "        ARGS:\n",
    "            TODO: fill in when I'm not in crunchtime\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.originals is not None\n",
    "        adversarials = Variable(self.adversarial_tensors())\n",
    "        originals = Variable(self.originals)\n",
    "        adv_labels = []\n",
    "        out_labels = []\n",
    "        adv_out = torch.max(classifier_net(normalizer(adversarials)), 1)[1]\n",
    "        adv_labels.extend(adv_out)\n",
    "        out = torch.max(classifier_net(normalizer(originals)), 1)[1]\n",
    "        out_labels.extend(out)\n",
    "        adv_idx_bytes = adv_out != out\n",
    "        idxs = []\n",
    "        for idx, el in enumerate(adv_idx_bytes):\n",
    "#             if float(el) > 0:\n",
    "            idxs.append(idx)\n",
    "\n",
    "        idxs = torch.LongTensor(idxs)\n",
    "        if self.originals.is_cuda:\n",
    "            idxs = idxs.cuda()\n",
    "\n",
    "        return [self.adversarial_tensors(), adv_labels,\n",
    "                self.originals(), out_labels]\n",
    "    \n",
    "    @initialized\n",
    "    def collect_successful(self, classifier_net, normalizer):\n",
    "        \"\"\" Returns a list of [adversarials, originals] of the SUCCESSFUL\n",
    "            attacks only, according to the given classifier_net, normalizer\n",
    "            SUCCESSFUL here means that the adversarial is different\n",
    "        ARGS:\n",
    "            TODO: fill in when I'm not in crunchtime\n",
    "        \"\"\"\n",
    "\n",
    "#         assert self.originals is not None\n",
    "#         adversarials = Variable(self.adversarial_tensors())\n",
    "#         originals = Variable(self.originals)\n",
    "\n",
    "#         adv_out = torch.max(classifier_net(normalizer(adversarials)), 1)[1]\n",
    "#         out = torch.max(classifier_net(normalizer(originals)), 1)[1]\n",
    "#         adv_idx_bytes = adv_out != out\n",
    "#         idxs = []\n",
    "#         for idx, el in enumerate(adv_idx_bytes):\n",
    "#             if float(el) > 0:\n",
    "#                 idxs.append(idx)\n",
    "\n",
    "#         idxs = torch.LongTensor(idxs)\n",
    "#         if self.originals.is_cuda:\n",
    "#             idxs = idxs.cuda()\n",
    "\n",
    "#         return [torch.index_select(self.adversarial_tensors(), 0, idxs),\n",
    "#                 torch.index_select(self.originals, 0, idxs)]\n",
    "\n",
    "        assert self.originals is not None\n",
    "        adversarials = Variable(self.adversarial_tensors())\n",
    "        originals = Variable(self.originals)\n",
    "        adv_labels = []\n",
    "        out_labels = []\n",
    "        adv_out = torch.max(classifier_net(normalizer(adversarials)), 1)[1]\n",
    "        adv_labels.extend(adv_out)\n",
    "        out = torch.max(classifier_net(normalizer(originals)), 1)[1]\n",
    "        out_labels.extend(out)\n",
    "        adv_idx_bytes = adv_out != out\n",
    "        idxs = []\n",
    "        for idx, el in enumerate(adv_idx_bytes):\n",
    "#             if float(el) > 0:\n",
    "            idxs.append(idx)\n",
    "\n",
    "        idxs = torch.LongTensor(idxs)\n",
    "        if self.originals.is_cuda:\n",
    "            idxs = idxs.cuda()\n",
    "        return [torch.index_select(self.adversarial_tensors(), 0, idxs), adv_labels,torch.index_select(self.originals, 0, idxs), out_labels]\n",
    "\n",
    "    @initialized\n",
    "    def collect_adversarially_successful(self, classifier_net, normalizer,\n",
    "                                         labels):\n",
    "        \"\"\" Returns an object containing the SUCCESSFUL attacked examples,\n",
    "            their corresponding originals, and the number of misclassified\n",
    "            examples\n",
    "        ARGS:\n",
    "            classifier_net : nn.Module subclass - neural net that is the\n",
    "                             relevant classifier\n",
    "            normalizer : DifferentiableNormalize object - object to convert\n",
    "                         input data to mean-zero, unit-var examples\n",
    "            labels : Variable (longTensor N) - correct labels for classification\n",
    "                     of self.originals\n",
    "        RETURNS:\n",
    "            dict with structure:\n",
    "            {'adversarials': Variable(N'xCxHxW) - adversarial perturbation\n",
    "                            applied\n",
    "             'originals': Variable(N'xCxHxW) - unperturbed examples that\n",
    "                                               were correctly classified AND\n",
    "                                               successfully attacked\n",
    "             'num_correctly_classified': int - number of correctly classified\n",
    "                                               unperturbed examples\n",
    "            }\n",
    "        \"\"\"\n",
    "        assert self.originals is not None\n",
    "        adversarials = Variable(self.adversarial_tensors())\n",
    "        originals = Variable(self.originals)\n",
    "\n",
    "        adv_out = torch.max(classifier_net(normalizer(adversarials)), 1)[1]\n",
    "        out = torch.max(classifier_net(normalizer(originals)), 1)[1]\n",
    "\n",
    "        # First take a subset of correctly classified originals\n",
    "        correct_idxs = (out == labels) # correctly classified idxs\n",
    "        adv_idx_bytes = (adv_out != out) # attacked examples\n",
    "\n",
    "        num_correctly_classified = int(sum(correct_idxs))\n",
    "\n",
    "        adv_idxs = adv_idx_bytes * correct_idxs\n",
    "\n",
    "\n",
    "        idxs = []\n",
    "        for idx, el in enumerate(adv_idxs):\n",
    "            if float(el) > 0:\n",
    "                idxs.append(idx)\n",
    "\n",
    "        idxs = torch.LongTensor(idxs)\n",
    "        if self.originals.is_cuda:\n",
    "            idxs = idxs.cuda()\n",
    "\n",
    "\n",
    "        return {'adversarial': torch.index_select(self.adversarial_tensors(),\n",
    "                                                  0, idxs),\n",
    "                'originals': torch.index_select(self.originals, 0, idxs),\n",
    "                'num_correctly_classified': num_correctly_classified}\n",
    "\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def display(self, scale=5, successful_only=False, classifier_net=None,\n",
    "                normalizer=None):\n",
    "        \"\"\" Displays this adversarial perturbation in a 3-row format:\n",
    "            top row is adversarial images, second row is original images,\n",
    "            bottom row is difference magnified by scale (default 5)\n",
    "        ARGS:\n",
    "            scale: int - how much to magnify differences by\n",
    "            successful_only: bool - if True we only display successful (in that\n",
    "                             advs output different classifier outputs)\n",
    "                             If this is not None, classifie_net and normalizer\n",
    "                             cannot be None\n",
    "        RETURNS:\n",
    "            None, but displays images\n",
    "        \"\"\"\n",
    "        if successful_only:\n",
    "            assert classifier_net is not None\n",
    "            assert normalizer is not None\n",
    "            advs, origs = self.collect_successful(classifier_net, normalizer)\n",
    "        else:\n",
    "            advs = self.adversarial_tensors()\n",
    "            origs = self.originals\n",
    "\n",
    "        diffs = torch.clamp((advs - origs) * scale + 0.5, 0, 1)\n",
    "        img_utils.show_images([advs, origs, diffs])\n",
    "\n",
    "\n",
    "class PerturbationParameters(dict):\n",
    "    \"\"\" Object that stores parameters like a dictionary.\n",
    "        This allows perturbation classes to be only partially instantiated and\n",
    "        then fed various 'originals' later.\n",
    "    Implementation taken from : https://stackoverflow.com/a/14620633/3837607\n",
    "    (and then modified with the getattribute trick to return none instead of\n",
    "     error for missing attributes)\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(PerturbationParameters, self).__init__(*args, **kwargs)\n",
    "        if kwargs.get('manual_gpu') is not None:\n",
    "            self.use_gpu = kwargs['manual_gpu']\n",
    "        else:\n",
    "            self.use_gpu = utils.use_gpu()\n",
    "        self.__dict__ = self\n",
    "\n",
    "    def __getattribute__(self, name):\n",
    "        try:\n",
    "            return object.__getattribute__(self, name)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "\n",
    "class ThreatModel(object):\n",
    "    def __init__(self, perturbation_class, param_kwargs, *other_args):\n",
    "        \"\"\" Factory class to generate per_minibatch instances of Adversarial\n",
    "            perturbations.\n",
    "        ARGS:\n",
    "            perturbation_class : class - subclass of Adversarial Perturbations\n",
    "            param_kwargs : dict - dict containing named kwargs to instantiate\n",
    "                           the class in perturbation class\n",
    "        \"\"\"\n",
    "        assert issubclass(perturbation_class, AdversarialPerturbation)\n",
    "        self.perturbation_class = perturbation_class\n",
    "        if isinstance(param_kwargs, dict):\n",
    "            param_kwargs = PerturbationParameters(**param_kwargs)\n",
    "        self.param_kwargs = param_kwargs\n",
    "        self.other_args = other_args\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"[Threat] %s: %s\"  % (str(self.perturbation_class.__name__),\n",
    "                                     self.param_kwargs)\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if args == ():\n",
    "            return self.perturbation_obj()\n",
    "        else:\n",
    "            perturbation_obj = self.perturbation_obj()\n",
    "            perturbation_obj.setup(*args)\n",
    "            return perturbation_obj\n",
    "\n",
    "\n",
    "\n",
    "    def perturbation_obj(self):\n",
    "        return self.perturbation_class(self, self.param_kwargs, *self.other_args)\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                            ADDITION PARAMETERS                             #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "class DeltaAddition(AdversarialPerturbation):\n",
    "\n",
    "    def __init__(self, threat_model, perturbation_params, *other_args):\n",
    "        \"\"\" Maintains a delta that gets addded to the originals to generate\n",
    "            adversarial images. This is the type of adversarial perturbation\n",
    "            that the literature extensivey studies\n",
    "        ARGS:\n",
    "            threat_model : ThreatModel object that is used to initialize self\n",
    "            perturbation_params: PerturbationParameters object.\n",
    "                { lp_style : None, int or 'inf' - if not None is the type of\n",
    "                            Lp_bound that we apply to this adversarial example\n",
    "                lp_bound : None or float - cannot be None if lp_style is\n",
    "                           not None, but if not None should be the lp bound\n",
    "                           we allow for adversarial perturbations\n",
    "                custom_norm : None or fxn:(NxCxHxW) -> Scalar Variable. This is\n",
    "                              not implemented for now\n",
    "                }\n",
    "        \"\"\"\n",
    "\n",
    "        super(DeltaAddition, self).__init__(threat_model, perturbation_params)\n",
    "        self.lp_style = perturbation_params.lp_style\n",
    "        self.lp_bound = perturbation_params.lp_bound\n",
    "        if perturbation_params.custom_norm is not None:\n",
    "            raise NotImplementedError(\"Only LP norms allowed for now\")\n",
    "        self.scalar_step = perturbation_params.scalar_step or 1.0\n",
    "\n",
    "\n",
    "    def _merge_setup(self, num_examples, delta_data):\n",
    "        \"\"\" DANGEROUS TO BE CALLED OUTSIDE OF THIS FILE!!!\"\"\"\n",
    "        self.num_examples = num_examples\n",
    "        self.delta = nn.Parameter(delta_data)\n",
    "        self.initialized = True\n",
    "\n",
    "\n",
    "    def setup(self, x):\n",
    "        super(DeltaAddition, self).setup(x)\n",
    "        self.delta = nn.Parameter(torch.zeros_like(x))\n",
    "        self.initialized = True\n",
    "\n",
    "    @initialized\n",
    "    def perturbation_norm(self, x=None, lp_style=None):\n",
    "        lp_style = lp_style or self.lp_style\n",
    "        assert isinstance(lp_style, int) or lp_style == 'inf'\n",
    "        return utils.batchwise_norm(self.delta, lp=lp_style)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def constrain_params(self):\n",
    "        new_delta = utils.batchwise_lp_project(self.delta.data, self.lp_style,\n",
    "                                               self.lp_bound)\n",
    "        delta_diff = new_delta - self.delta.data\n",
    "        self.delta.data.add_(delta_diff)\n",
    "\n",
    "    @initialized\n",
    "    def make_valid_image(self, x):\n",
    "        new_delta = self.delta.data\n",
    "        change_in_delta = utils.clamp_0_1_delta(new_delta, x)\n",
    "        self.delta.data.add_(change_in_delta)\n",
    "\n",
    "    @initialized\n",
    "    def update_params(self, step_fxn):\n",
    "        assert self.delta.grad.data is not None\n",
    "        self.add_to_params(step_fxn(self.delta.grad.data) * self.scalar_step)\n",
    "\n",
    "    @initialized\n",
    "    def add_to_params(self, grad_data):\n",
    "        \"\"\" sets params to be self.params + grad_data \"\"\"\n",
    "        self.delta.data.add_(grad_data)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def random_init(self):\n",
    "        self.delta = nn.Parameter(utils.random_from_lp_ball(self.delta.data,\n",
    "                                                            self.lp_style,\n",
    "                                                            self.lp_bound))\n",
    "\n",
    "    @initialized\n",
    "    def merge_perturbation(self, other, self_mask):\n",
    "        super(DeltaAddition, self).merge_perturbation(other, self_mask)\n",
    "\n",
    "        # initialize a new perturbation\n",
    "        new_perturbation = DeltaAddition(self.threat_model,\n",
    "                                         self.perturbation_params)\n",
    "\n",
    "        # make the new parameters\n",
    "        new_delta = utils.fold_mask(self.delta.data, other.delta.data,\n",
    "                                    self_mask)\n",
    "\n",
    "        # do the merge setup and return the object\n",
    "        new_perturbation._merge_setup(self.num_examples,\n",
    "                                      new_delta)\n",
    "        return new_perturbation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self.setup(x)\n",
    "        self.make_valid_image(x) # not sure which one to do first...\n",
    "        self.constrain_params()\n",
    "        return x + self.delta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                               SPATIAL PARAMETERS                           #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "class ParameterizedXformAdv(AdversarialPerturbation):\n",
    "\n",
    "    def __init__(self, threat_model, perturbation_params, *other_args):\n",
    "        super(ParameterizedXformAdv, self).__init__(threat_model,\n",
    "                                                    perturbation_params)\n",
    "        assert issubclass(perturbation_params.xform_class,\n",
    "                          st.ParameterizedTransformation)\n",
    "\n",
    "        self.lp_style = perturbation_params.lp_style\n",
    "        self.lp_bound = perturbation_params.lp_bound\n",
    "        self.use_stadv = perturbation_params.use_stadv\n",
    "        self.scalar_step = perturbation_params.scalar_step or 1.0\n",
    "\n",
    "\n",
    "    def _merge_setup(self, num_examples, new_xform):\n",
    "        \"\"\" DANGEROUS TO BE CALLED OUTSIDE OF THIS FILE!!!\"\"\"\n",
    "        self.num_examples = num_examples\n",
    "        self.xform = new_xform\n",
    "        self.initialized = True\n",
    "\n",
    "    def setup(self, originals):\n",
    "        super(ParameterizedXformAdv, self).setup(originals)\n",
    "        self.xform = self.perturbation_params.xform_class(shape=originals.shape,\n",
    "                                                        manual_gpu=self.use_gpu)\n",
    "        self.initialized = True\n",
    "\n",
    "    @initialized\n",
    "    def perturbation_norm(self, x=None, lp_style=None):\n",
    "        lp_style = lp_style or self.lp_style\n",
    "        if self.use_stadv is not None:\n",
    "            assert isinstance(self.xform, st.FullSpatial)\n",
    "            return self.xform.stAdv_norm()\n",
    "        else:\n",
    "            return self.xform.norm(lp=lp_style)\n",
    "\n",
    "    @initialized\n",
    "    def constrain_params(self, x=None):\n",
    "        # Do lp projections\n",
    "        if isinstance(self.lp_style, int) or self.lp_style == 'inf':\n",
    "            self.xform.project_params(self.lp_style, self.lp_bound)\n",
    "\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def update_params(self, step_fxn):\n",
    "        param_list = list(self.xform.parameters())\n",
    "        assert len(param_list) == 1\n",
    "        params = param_list[0]\n",
    "        assert params.grad.data is not None\n",
    "        self.add_to_params(step_fxn(params.grad.data) * self.scalar_step)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def add_to_params(self, grad_data):\n",
    "        \"\"\" Assumes only one parameters object in the Spatial Transform \"\"\"\n",
    "        param_list = list(self.xform.parameters())\n",
    "        assert len(param_list) == 1\n",
    "        params = param_list[0]\n",
    "        params.data.add_(grad_data)\n",
    "\n",
    "    @initialized\n",
    "    def random_init(self):\n",
    "        param_list = list(self.xform.parameters())\n",
    "        assert len(param_list) == 1\n",
    "        param = param_list[0]\n",
    "        random_perturb = utils.random_from_lp_ball(param.data,\n",
    "                                                   self.lp_style,\n",
    "                                                   self.lp_bound)\n",
    "\n",
    "        param.data.add_(self.xform.identity_params(self.xform.img_shape) +\n",
    "                        random_perturb - self.xform.xform_params.data)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def merge_perturbation(self, other, self_mask):\n",
    "        super(ParameterizedXformAdv, self).merge_perturbation(other, self_mask)\n",
    "        new_perturbation = ParameterizedXformAdv(self.threat_model,\n",
    "                                                 self.perturbation_params)\n",
    "\n",
    "        new_xform = self.xform.merge_xform(other.xform, self_mask)\n",
    "        new_perturbation._merge_setup(self.num_examples, new_xform)\n",
    "\n",
    "        return new_perturbation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self.setup(x)\n",
    "        self.constrain_params()\n",
    "        return self.xform.forward(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                                                                            #\n",
    "#                            SPATIAL + ADDITION PARAMETERS                   #\n",
    "#                                                                            #\n",
    "##############################################################################\n",
    "\n",
    "class SequentialPerturbation(AdversarialPerturbation):\n",
    "    \"\"\" Takes a list of perturbations and composes them. A norm needs to\n",
    "        be specified here to describe the perturbations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threat_model, perturbation_sequence,\n",
    "                 global_parameters=PerturbationParameters(pad=10),\n",
    "                 preinit_pipeline=None):\n",
    "        \"\"\" Initializes a sequence of adversarial perturbation layers\n",
    "        ARGS:\n",
    "            originals : NxCxHxW tensor - original images we create adversarial\n",
    "                        perturbations for\n",
    "            perturbation_sequence : ThreatModel[]  -\n",
    "                list of ThreatModel objects\n",
    "            global_parameters : PerturbationParameters - global parameters to\n",
    "                                use. These contain things like how to norm this\n",
    "                                sequence, how to constrain this sequence, etc\n",
    "            preinit_pipelines: list[]\n",
    "                if not None i\n",
    "         \"\"\"\n",
    "        super(SequentialPerturbation, self).__init__(threat_model,\n",
    "                                                    (perturbation_sequence,\n",
    "                                                     global_parameters))\n",
    "\n",
    "        if preinit_pipeline is not None:\n",
    "            layers = preinit_pipeline\n",
    "        else:\n",
    "            layers = []\n",
    "            for threat_model in perturbation_sequence:\n",
    "                assert isinstance(threat_model, ThreatModel)\n",
    "                layers.append(threat_model())\n",
    "\n",
    "        self.pipeline = []\n",
    "        for layer_no, layer in enumerate(layers):\n",
    "            self.pipeline.append(layer)\n",
    "            self.add_module('layer_%02d' % layer_no, layer)\n",
    "\n",
    "\n",
    "        # norm: pipeline -> Scalar Variable\n",
    "        self.norm = global_parameters.norm\n",
    "        self.norm_weights = global_parameters.norm_weights\n",
    "\n",
    "        # padding with black is useful to not throw information away during\n",
    "        # sequential steps\n",
    "        self.pad = nn.ConstantPad2d(global_parameters.pad or 0, 0)\n",
    "        self.unpad = nn.ConstantPad2d(-1 * (global_parameters.pad or 0), 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _merge_setup(self, num_examples):\n",
    "        self.num_examples = num_examples\n",
    "        self.initialized = True\n",
    "\n",
    "\n",
    "    def setup(self, x):\n",
    "        super(SequentialPerturbation, self).setup(x)\n",
    "        x = self.pad(x)\n",
    "        for layer in self.pipeline:\n",
    "            layer.setup(x)\n",
    "        self.initialized = True\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def perturbation_norm(self, x=None, lp_style=None):\n",
    "        # Need to define a nice way to describe the norm here. This can be\n",
    "        # an empirical norm between input/output\n",
    "        # For now, let's just say it's the sum of the norms of each constituent\n",
    "        if self.norm is not None:\n",
    "            return self.norm(self.pipeline, x=x, lp_style=lp_style)\n",
    "        else:\n",
    "            norm_weights = self.norm_weights or\\\n",
    "                              [1.0 for _ in range(len(self.pipeline))]\n",
    "            out = None\n",
    "            for i, layer in enumerate(self.pipeline):\n",
    "                weight = norm_weights[i]\n",
    "                layer_norm = layer.perturbation_norm(x=x, lp_style=lp_style)\n",
    "                if out is None:\n",
    "                    out = layer_norm * weight\n",
    "                else:\n",
    "                    out = out + layer_norm * weight\n",
    "            return out\n",
    "\n",
    "    @initialized\n",
    "    def make_valid_image(self, x):\n",
    "        x = self.pad(x)\n",
    "        for layer in self.pipeline:\n",
    "            layer.make_valid_image(x)\n",
    "            x = layer(x)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def constrain_params(self):\n",
    "        # Need to do some sort of crazy projection operator for general things\n",
    "        # For now, let's just constrain each thing in sequence\n",
    "\n",
    "        for layer in self.pipeline:\n",
    "            layer.constrain_params()\n",
    "\n",
    "    @initialized\n",
    "    def update_params(self, step_fxn):\n",
    "        for layer in self.pipeline:\n",
    "            layer.update_params(step_fxn)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def merge_perturbation(self, other, self_mask):\n",
    "        super(SequentialPerturbation, self).merge_perturbation(other, self_mask)\n",
    "\n",
    "\n",
    "        new_pipeline = []\n",
    "        for self_layer, other_layer in zip(self.pipeline, other.pipeline):\n",
    "            new_pipeline.append(self_layer.merge_perturbation(other_layer,\n",
    "                                                              self_mask))\n",
    "\n",
    "\n",
    "        layer_params, global_params = self.perturbation_params\n",
    "\n",
    "        new_perturbation = SequentialPerturbation(self.threat_model,\n",
    "                                                layer_params,\n",
    "                                                global_parameters=global_params,\n",
    "                                                preinit_pipeline=new_pipeline)\n",
    "        new_perturbation._merge_setup(self.num_examples)\n",
    "\n",
    "        return new_perturbation\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, layer_slice=None):\n",
    "        \"\"\" Layer slice here is either an int or a tuple\n",
    "        If int, we run forward only the first layer_slice layers\n",
    "        If tuple, we start at the\n",
    "        \"\"\"\n",
    "\n",
    "        # Blocks to handle only particular layer slices (debugging)\n",
    "        if layer_slice is None:\n",
    "            pipeline_iter = iter(self.pipeline)\n",
    "        elif isinstance(layer_slice, int):\n",
    "            pipeline_iter = iter(self.pipeline[:layer_slice])\n",
    "        elif isinstance(layer_slice, tuple):\n",
    "            pipeline_iter = iter(self.pipeline[layer_slice[0]: layer_slice[1]])\n",
    "        # End block to handle particular layer slices\n",
    "\n",
    "        # Handle padding\n",
    "        original_hw = x.shape[-2:]\n",
    "        if not self.initialized:\n",
    "            self.setup(x)\n",
    "\n",
    "        self.constrain_params()\n",
    "        self.make_valid_image(x)\n",
    "\n",
    "        x = self.pad(x)\n",
    "        for layer in pipeline_iter:\n",
    "            x = layer(x)\n",
    "        return self.unpad(x)\n",
    "\n",
    "\n",
    "    @initialized\n",
    "    def random_init(self):\n",
    "        for layer in self.pipeline:\n",
    "            layer.random_init()\n",
    "\n",
    "    @initialized\n",
    "    def attach_originals(self, originals):\n",
    "        self.originals = originals\n",
    "        for layer in self.pipeline:\n",
    "            layer.attach_originals(originals)\n",
    "            \n",
    "class ReColorAdv(AdversarialPerturbation):\n",
    "    \"\"\"\n",
    "    Puts the color at each pixel in the image through the same transformation.\n",
    "    Parameters:\n",
    "     - lp_style: number or 'inf'\n",
    "     - lp_bound: maximum norm of color transformation. Can be a tensor of size\n",
    "       (num_channels,), in which case each channel will be bounded by the\n",
    "       cooresponding bound in the tensor. For instance, passing\n",
    "       [0.1, 0.15, 0.05] would allow a norm of 0.1 for R, 0.15 for G, and 0.05\n",
    "       for B. Not supported by all transformations.\n",
    "     - use_smooth_loss: whether to optimize using the loss function\n",
    "       for FullSpatial that rewards smooth vector fields\n",
    "     - xform_class: a subclass of\n",
    "       color_transformers.ParameterizedTransformation\n",
    "     - xform_params: dict of parameters to pass to the xform_class.\n",
    "     - cspace_class: a subclass of color_spaces.ColorSpace that indicates\n",
    "       in which color space the transformation should be performed\n",
    "       (RGB by default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threat_model, perturbation_params, *other_args):\n",
    "        super().__init__(threat_model, perturbation_params)\n",
    "        assert issubclass(perturbation_params.xform_class,\n",
    "                          ct.ParameterizedTransformation)\n",
    "\n",
    "        self.lp_style = perturbation_params.lp_style\n",
    "        self.lp_bound = perturbation_params.lp_bound\n",
    "        self.use_smooth_loss = perturbation_params.use_smooth_loss\n",
    "        self.scalar_step = perturbation_params.scalar_step or 1.0\n",
    "        self.cspace = perturbation_params.cspace or cs.RGBColorSpace()\n",
    "\n",
    "    def _merge_setup(self, num_examples, new_xform):\n",
    "        \"\"\" DANGEROUS TO BE CALLED OUTSIDE OF THIS FILE!!!\"\"\"\n",
    "        self.num_examples = num_examples\n",
    "        self.xform = new_xform\n",
    "        self.initialized = True\n",
    "\n",
    "    def setup(self, originals):\n",
    "        super().setup(originals)\n",
    "        self.xform = self.perturbation_params.xform_class(\n",
    "            shape=originals.shape, manual_gpu=self.use_gpu,\n",
    "            cspace=self.cspace,\n",
    "            **(self.perturbation_params.xform_params or {}),\n",
    "        )\n",
    "        self.initialized = True\n",
    "\n",
    "    @initialized\n",
    "    def perturbation_norm(self, x=None, lp_style=None):\n",
    "        lp_style = lp_style or self.lp_style\n",
    "        if self.use_smooth_loss:\n",
    "            assert isinstance(self.xform, ct.FullSpatial)\n",
    "            return self.xform.smoothness_norm()\n",
    "        else:\n",
    "            return self.xform.norm(lp=lp_style)\n",
    "\n",
    "    @initialized\n",
    "    def constrain_params(self, x=None):\n",
    "        # Do lp projections\n",
    "        if isinstance(self.lp_style, int) or self.lp_style == 'inf':\n",
    "            self.xform.project_params(self.lp_style, self.lp_bound)\n",
    "\n",
    "    @initialized\n",
    "    def update_params(self, step_fxn):\n",
    "        param_list = list(self.xform.parameters())\n",
    "        assert len(param_list) == 1\n",
    "        params = param_list[0]\n",
    "        assert params.grad.data is not None\n",
    "        self.add_to_params(step_fxn(params.grad.data) * self.scalar_step)\n",
    "\n",
    "    @initialized\n",
    "    def add_to_params(self, grad_data):\n",
    "        \"\"\" Assumes only one parameters object in the Spatial Transform \"\"\"\n",
    "        param_list = list(self.xform.parameters())\n",
    "        assert len(param_list) == 1\n",
    "        params = param_list[0]\n",
    "        params.data.add_(grad_data)\n",
    "\n",
    "    @initialized\n",
    "    def random_init(self):\n",
    "        param_list = list(self.xform.parameters())\n",
    "        assert len(param_list) == 1\n",
    "        param = param_list[0]\n",
    "        random_perturb = utils.random_from_lp_ball(param.data,\n",
    "                                                   self.lp_style,\n",
    "                                                   self.lp_bound)\n",
    "\n",
    "        param.data.add_(self.xform.identity_params +\n",
    "                        random_perturb - self.xform.xform_params.data)\n",
    "\n",
    "    @initialized\n",
    "    def merge_perturbation(self, other, self_mask):\n",
    "        super().merge_perturbation(other, self_mask)\n",
    "        new_perturbation = ReColorAdv(self.threat_model,\n",
    "                                      self.perturbation_params)\n",
    "\n",
    "        new_xform = self.xform.merge_xform(other.xform, self_mask)\n",
    "        new_perturbation._merge_setup(self.num_examples, new_xform)\n",
    "\n",
    "        return new_perturbation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self.setup(x)\n",
    "        self.constrain_params()\n",
    "\n",
    "        return self.cspace.to_rgb(\n",
    "            self.xform.forward(self.cspace.from_rgb(x)))\n",
    "\n",
    "def attack(models, normalizer, examples, labels):\n",
    "    new_model = copy.deepcopy(models)\n",
    "    recoloradv_threat = ThreatModel(ReColorAdv, {\n",
    "        'xform_class': ct.FullSpatial, \n",
    "        'cspace': cs.CIELUVColorSpace(), # controls the color space used\n",
    "        'lp_style': 'inf',\n",
    "        'lp_bound': [0.06, 0.06, 0.06],  # [epsilon_1, epsilon_2, epsilon_3]\n",
    "        'xform_params': {\n",
    "          'resolution_x': 16,            # R_1\n",
    "          'resolution_y': 32,            # R_2\n",
    "          'resolution_z': 32,            # R_3\n",
    "        },\n",
    "        'use_smooth_loss': True,\n",
    "    })\n",
    "\n",
    "\n",
    "    # Now, we define the main optimization term (the Carlini & Wagner f6 loss).\n",
    "    adv_loss = CWLossF6(new_model, normalizer)\n",
    "    # adv_loss = CWLossF6(model)\n",
    "    # normalizer = None\n",
    "\n",
    "    # We also need the smoothness loss.\n",
    "    smooth_loss = PerturbationNormLoss(lp=2)\n",
    "\n",
    "    # We combine them with a RegularizedLoss object.\n",
    "    attack_loss = RegularizedLoss({'adv': adv_loss, 'smooth': smooth_loss}, \n",
    "                                     {'adv': 1.0,      'smooth': 0.05},   # lambda = 0.05\n",
    "                                     negate=True) # Need this true for PGD type attacks\n",
    "\n",
    "    # PGD is used to optimize the above loss.\n",
    "    pgd_attack_obj = aa.PGD(new_model, normalizer, recoloradv_threat, attack_loss)\n",
    "\n",
    "    # We run the attack for 10 iterations at learning rate 0.01.\n",
    "    perturbation = pgd_attack_obj.attack(examples, labels, num_iterations=random.randint(5,40), signed=False, \n",
    "                                         optimizer=optim.Adam, optimizer_kwargs={'lr': 0.01},\n",
    "                                         verbose=True)\n",
    "\n",
    "    # Now, we can collect the successful adversarial examples and display them.\n",
    "    # successful_advs, successful_origs = perturbation.collect_successful(model, normalizer)\n",
    "    all_advs, adv_labels, all_origs, out_labels = perturbation.collect_successful(new_model, normalizer)\n",
    "\n",
    "    return all_advs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f002dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and validation and test\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, normalizer, method = None):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if method == 'canny':\n",
    "            input = produceCanny(input)\n",
    "        elif method == 'sfd':\n",
    "            input = produceStructuredForest(input)\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "        input_var = normal(input_var)\n",
    "#         attacked_var = attack(model, normalizer, input_var, target)\n",
    "#         if args.half:\n",
    "#             input_var = input_var.half()\n",
    "\n",
    "        # compute output\n",
    "#         output1 = model(attacked_var)\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "#         model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, normalizer, method = None):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            if method == 'canny':\n",
    "                input = produceCanny(input)\n",
    "            elif method == 'sfd':\n",
    "                input = produceStructuredForest(input)\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "        #         input_var = [transform(img) for img in input_var]\n",
    "        #         input_var = torch.stack(input_var).cuda()\n",
    "        #         attacked_var = attack(model, normalizer, input_var, target_var)\n",
    "        #         attacked_var = normal(attacked_var)\n",
    "            input_var = normal(input_var)\n",
    "        #             if args.half:\n",
    "        #                 input_var = input_var.half()\n",
    "\n",
    "            # compute output\n",
    "        #         model.zero_grad()\n",
    "            output = model(input_var)\n",
    "        #         output = model(attacked_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc5cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "##producing canny images and structuredForest detection images\n",
    "#the idea is that the attack changes the color of the images a little bit, so we make the images black and white\n",
    "\n",
    "def produceCanny(images):\n",
    "    newImages = []\n",
    "    for image in images:\n",
    "        m = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        image = m(image.unsqueeze(0))\n",
    "        image = image.squeeze().cpu()\n",
    "        gray = cv2.cvtColor(image.permute(1,2,0).numpy(), cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        blurred = np.uint8(blurred*255)\n",
    "        mid = cv2.Canny(blurred, 30,150)\n",
    "        mid = np.stack((mid,)*3, -1)\n",
    "        mid = transform(mid)\n",
    "        newImages.append(mid)\n",
    "    return torch.stack(newImages)\n",
    "\n",
    "def produceStructuredForest(images):\n",
    "    newImages = []\n",
    "    edge_detector = cv2.ximgproc.createStructuredEdgeDetection('model.yml')\n",
    "    for image in images:\n",
    "        m = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        image = m(image.unsqueeze(0))\n",
    "        image = image.squeeze()\n",
    "        image = image.permute(1,2,0)\n",
    "        d = np.float32(image)\n",
    "        edges = edge_detector.detectEdges(d)\n",
    "        image = edges.astype('f')\n",
    "        image = np.stack((image,)*3, -1)\n",
    "        image = np.uint8(image*255)\n",
    "        image = transform(image)\n",
    "        newImages.append(image)\n",
    "    return torch.stack(newImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8234b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##change the model here\n",
    "\n",
    "model = torch.nn.DataParallel(resnet32())\n",
    "model.cuda()\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cd01794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "CIFAR10_MEANS = [0.485, 0.456, 0.406]\n",
    "CIFAR10_STDS = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalizer = utils.DifferentiableNormalize(mean=CIFAR10_MEANS,\n",
    "                                                   std=CIFAR10_STDS)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "normal = transforms.Compose([\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "#         normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=32, shuffle=True,\n",
    "    num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         normalize,\n",
    "    ])),\n",
    "    batch_size=16, shuffle=False,\n",
    "    num_workers=2, pin_memory=True)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# if args.half:\n",
    "#     model.half()\n",
    "#     criterion.half()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.05, momentum = 0.9, weight_decay = 1e-4)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                    milestones=[100, 150], last_epoch= 0 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfeab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, labels = next(iter(val_loader))\n",
    "examples = examples.cuda()\n",
    "labels = labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7229b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(START):  12.5 correct\n",
      "(iteration 00):  12.5 correct\n",
      "(iteration 01):  12.5 correct\n",
      "(iteration 02):  12.5 correct\n",
      "(iteration 03):  12.5 correct\n",
      "(iteration 04):  12.5 correct\n",
      "(iteration 05):  12.5 correct\n"
     ]
    }
   ],
   "source": [
    "adv_image_vanilla = attack(model, normalizer, examples, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "618d643f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAe8klEQVR4nO2da4yc53Xf/2feuex9Z5dLLpdXURJlRVZiSqFVO1EV2akDRUkgGwhcu4ChAEYUBBEQA+kHwQVqF+gHp6ht+EPhgq5VK4ZrWbUtSEiE1LYcRDDsSKJu1IW6ULxIJJdcksu97+zcTj/MyKXU5//sksudpf38fwDB2efs875nnnnPvLPPf8455u4QQvz6k1tvB4QQnUHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkQn41k83sDgBfA5AB+B/u/qXY7/d3533DQDF8rPh5Ltq3mKTo4Lbouci06PH40eJGj70Px/wP2yx2MjIHAGLK7KXJttyP2NHcL/4aaB2TrQenGX3Sl+ZH7NkxSzPiBvNxer6OxaVG0MlLDnYzywD8NwAfA3AcwNNm9qi7v8LmbBgo4gv/7vrw8bxJz1UshN20HA+IanWJ2uqNGj9XMfxmBACNZthHj7wqlmtQWy6jJnitlx8T/JiFYiU4nkVeastx/xvNOrXV6vw1azZJUBj3ox6+RgEAS+x4WC5wwz7G3tSrVX59NBqRdYxcw7nIa1Yl19U8X3osVMPH+/ZPTkR8uHRuAXDI3Q+7exXAgwDuWsXxhBBryGqCfSuAty/4+Xh7TAhxBbLmG3Rmdo+Z7Tez/XOLkc8lQog1ZTXBfgLA9gt+3tYeexfuvs/d97r73r7uVe0HCiFWwWqC/WkAu81sl5kVAXwKwKOXxy0hxOXmkm+17l43s3sB/B+0pLf73f3l6BwYquT9xX2RTyS7lSXwHesc+FZ3Ph/ZIb8ExcsKfNJStUpt9WbEx4j0lkV28fNkmjX5DjPqXLmI7SI3I/5XrSs43shKfE7seA2+HtbkPhpRE7oir1neuC2XjygXtcgaG/8T1skae0RnyLKwjzFlYlWfq939MQCPreYYQojOoG/QCZEICnYhEkHBLkQiKNiFSAQFuxCJ0OFvuTicJVY4l3+8EZ5jDS7VNGtc8sq6IzIOeDIDk7yaEemnWChQW925rVmLPLfI+er1sM0imVy5iMxnGU8M8iwsrwHAYiMssZ06x+Wp+Sr3cW6Oz8ucr0d/V3gdi8Zf54GebmrrLnEJrZnj11wuKqOFfeRXB1BjyVcR7U13diESQcEuRCIo2IVIBAW7EImgYBciETq6G2/uyDfIrnsW2S0mSRylLJIfn49tS0YSHUiCAQCaCFOPFQvLcT8KRb7ru/mq66htZuostZ09txA+V57vqucQSU6p80tk0bn/B4+FffTSMJ1Ty3hiU7WP7/zPTU9S24mJqeB4X4k/r8ap8BwA2DHK13FDP1/HrnysnFX4Oi5GLuEGUSBi5bZ0ZxciERTsQiSCgl2IRFCwC5EICnYhEkHBLkQirEO517A0YPkyn0HkhHqsA0eOy3LVOk9YKEZqpDUapFZYJDEFESmkGKmD9q/+zceo7Zmf/4LaTk6dC47PRyS0eoNLXseOn6G2Iyd495FSeSw4vm10F53jpX5qq+b561Lo20ht9cpccPzcxEk6p6fM5cHjc6eprUJqJQLAaD9Pa+kphBNhGrWwjAoArIlPpJOX7uxCpIKCXYhEULALkQgKdiESQcEuRCIo2IVIhFVJb2Z2FMAsgAaAurvvjf1+03JYyoXllemFHjqvQdoTDfVxeW0g43JYPlKPrRmR5ZisQevqIZ5Ft7Bwntp++vePUNvpKV6v7/Rc+HzHTvBzHRt/m9qyrj5qa2QD1NY7MBIcL/Tw4+W7eBZdKdKSqSvHpcOz1XBbsbFtO+icyuI8tR05wqW3yekKtWXGn/dVG8O2QoNLecbqMkak3suhs3/E3XnOpRDiikAf44VIhNUGuwP4kZk9Y2b3XA6HhBBrw2o/xt/q7ifMbBOAH5vZq+7+xIW/0H4TuAcAhvp5lQ8hxNqyqju7u59o/z8B4GEAtwR+Z5+773X3vX3d6/BVfCEEgFUEu5n1mln/O48B/AGAly6XY0KIy8tqbrWjAB5ub/XnAfwvd//H2IR603BmMZzhM1kr03lP/Pyfg+O/sZtLLh95f1j6AYChSHHLJslsA4AcadOTy/GMpobztkURNQlHjh2htslFngHmPUPB8ayPSz+5oVlq6y4PUlu1wqWmKmmvNDDEX7OBPm6bOHWK2mbO84KT/cXwJd7VzWW+t85zcanQv4nazpx6i9r6TvM13jwQ9qXbIpmKpAgrIrLyJQe7ux8G8IFLnS+E6CyS3oRIBAW7EImgYBciERTsQiSCgl2IROhsr7eshPxguODgwjn+vlMrhgsKTi6EpTAAWKjy3mADRZ7Z1iR9t9rG4HCW8Yy9SpVLPGd48hrOznIJMFYQcWhjOJtrvjlD54yA+5hFMtGqBb6Olfmw1FSZ437sHN1AbQtEQgOACZLZBgBWCMuU05O8mCMiBUQX53lGXFbk18HEDM86HCfZcjtH+PWdYwlxsRaH3CSE+HVCwS5EIijYhUgEBbsQiaBgFyIROrob39Xdi/f91v+XBQsAOP4vr9F5fYPh3fhbPhw+FgD0ZMeorUp2igEgl+dJLVYI70w3vEzn9G/aTm3PHzhEbX1lvjO9def7qc1z4d3nQmTnvLkUbhkFANVqpMVWZK0yksTx8gsH6JyBUqRFUi9PkumN1LU7eSpcM65OlBUAyMgOPgAM9XN1YrrBk57OT3LbkVPTwfEto5vpnDxTlCLZVbqzC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhE6Kr3lsjx6BsOS0s6rr6PzFolqsWPXtXTOSI1LK1NHuCxXiyTCNOrhRIdbbvs4nbPjat4Ra9dvHqW2Z557gdqG+rgkc3IiXD8t77yMd6nAJS/wZcRcJClkmtSFG+rl54qcCo2IVDayMSzNAsBSLfx6nj0flrsAwCItu/ojdfLyGQ+naoUn3hx++3hwfGOZy3y7t4XbqHnk/q07uxCJoGAXIhEU7EIkgoJdiERQsAuRCAp2IRJhWenNzO4H8McAJtz9xvbYMIDvAbgKwFEAn3R3XmTrnWPlcshK4Qylk6cP0nl7fvuDwfHeQV7zK5s9QW2NeqRFTqTW2eG3w9lytw6F6+oBAHq2UVN/L5djuvI8k6s7Uuusq0gytiJ11bZuGaO2V958k9qKRV7nb2Y2vFZXbdtN51x3/Q3UNjnJL6++gTK1nTw1ERy3HK/vVh7iNf6mI7Xksohk191TprbF2fB1cIhcbwDQXQyfq1aPZClSy//jWwDueM/YfQAed/fdAB5v/yyEuIJZNtjb/dbf+w2JuwA80H78AICPX163hBCXm0v9m33U3cfbj0+h1dFVCHEFs+oNOnd3RL7paGb3mNl+M9s/Pc1rhgsh1pZLDfbTZjYGAO3/w7sgANx9n7vvdfe9g4MDl3g6IcRqudRgfxTA3e3HdwN45PK4I4RYK1YivX0XwO0ARszsOIAvAPgSgIfM7LMAjgH45EpOZpah0BW+u1cqvCDi0lI47a0QkaB6evmniN5IS6NSxrPe+vLhfk3f2vdNOudP/u291FaYP0VtxVIkeynHfdx19dbg+MTkSTqnMsez1zZvGqG2yRkuHS5Vw6/n1dfyTMVrruWZj9PPPUtt87Nz1DYzH/ax3uAS1eJiuB0TAJTLg9TWcC6VDZR5tl+9Gn49sxzvD3Z8PPxhukqy/IAVBLu7f5qYfn+5uUKIKwd9g06IRFCwC5EICnYhEkHBLkQiKNiFSISOFpyEGSwLSxALEfmnsrAYHC9EenLNnuNZXsi49FYAL0Q4Vg5nSr1xkPdsO3mc27DA5bBjx49S202beY+7rTvDxSi3TPBvNM8f4gU4h0tlausvc1nu8OGjwfGxLWFpEACmZvg3LGsRqez0Gd6rrukWHLdIcciFiPRmOX5dhc/UojdSqBLNcJZd0cLXPQBUz4VlW4+U7dSdXYhEULALkQgKdiESQcEuRCIo2IVIBAW7EInQWenNAZCeXZlzaWVsJNwfrqeLS28/PcALJQ5FivLtHubZSV2lsOxSzHOp5szEUWprLvHihTuu4UUss8jz7hkYCo6PjPLCl+cmedbYdCSzrRFRNzeS/mv5iFxaIdlfQDyba7HCs8PqxEk2DgCVJZ6BWa/z++OGkU3UZsavq6KFr5+SRfoOejjjsxApeqk7uxCJoGAXIhEU7EIkgoJdiERQsAuRCB3djTcDCvlwMslgH09OKfeHbdbku5UzzhMPzp7nKQsj/XxJeovhHdVGLlwjDwCOnjxKbaNDvJ7Zzmt5K6QKPx2eeibcRuvEON/57+8L7+ADQKHAWzy9fOgt7gi5jzQj95elyG783DxPCikP83ZNdZIIM36aFkRGbz9/XfIZTzTp6eE1EYusLRcA1MKJPI35KTpldFN/cDxf4G2tdGcXIhEU7EIkgoJdiERQsAuRCAp2IRJBwS5EIqyk/dP9AP4YwIS739ge+yKAPwdwpv1rn3f3x1ZywszCUsjmTeHaaS0niYwTSYAY28YTSfZH5LAp45KdZ+E6eYMjPKlicIAnQBS6wvIJAFwVkd76BsOJQQDwP+//dnB8IbJWM4uT1LawyGsDFiJXz+ah8POuTPJ6d/Mk0QgABgf46/Lqa29Q2+nTZ4LjM5GWUeUyf2IDvX3UljnXRAtVvo4ZqUW4sZcfb7ArHEf5yO17JXf2bwG4IzD+VXff0/63okAXQqwfywa7uz8BgL/1CyF+JVjN3+z3mtkBM7vfzPhXsIQQVwSXGuxfB3ANgD0AxgF8mf2imd1jZvvNbP/U1NQlnk4IsVouKdjd/bS7N9y9CeAbAGjXAnff5+573X1vuVy+RDeFEKvlkoLdzMYu+PETAF66PO4IIdaKlUhv3wVwO4ARMzsO4AsAbjezPWhVlTsK4C9WcrJcLkezfwaGuPRWb4TdLOV5JtF1u3ZQ2/5nuOQ1U7iW2po2Gxwf3crltVcO/gu1/c7v/Rm1/eLnfN78fKRNUvVscHzi1Nt0Tuw9f67GbXlwaWgoF86y29rNfZ8+wyW0esa3hUY3cVujEc6kW4y0eKos8rp785EaevUml/NqlRPUtqkQzujb0sez6Jbq4Tmxu/eywe7unw4Mf3O5eUKIKwt9g06IRFCwC5EICnYhEkHBLkQiKNiFSISOFpzM5XLo7QtnLw2NjNB5dQu7WckV6ZyuvgFqK5d5QcG33j5Fbbd+8P1hP+Z4O6me/nDWFQCMnzhObYdef53a6g3enihH6g3Oz0zTOf0bxqhteprLUIN9vBjl+667MTj+9Auv0jnPvnqU2m69/Q+prVDkEtXhQ4eC49Oz/HnFimJWFrm8tnOUS7rdvbyg6vBweJ7neQHOejVc+NJJVimgO7sQyaBgFyIRFOxCJIKCXYhEULALkQgKdiESoaPSm3sTzXpY8hgc5oX85hfDhQgXGrzvVpbx97Ed27dR2+sv88yr6YWwxNbXyzPstl9DTTj2Oi++eOLkOLV9+MMfpLaFhbA01L9lK50zvIUX53xrkktli0tcciz2hvuvDWzcTufc1M9flzNnwv3QAODosReobX4xLFNOTXMJbePGjdQ26Px12dnHJdFNA7wHW8HCmYDVGu9v10skthx4TOjOLkQiKNiFSAQFuxCJoGAXIhEU7EIkQkd345v1GmbPhXczuyO1vZYq4V1Oa3L3zfiu5Mgwb5/0eu4wtU1Mhlv4nMv4rvRgH6+td/2NPCHn8DFeM67GuyRhaiasduzevZvO2b2LSwbHxnkCzcsvv0ht586Gk1OKJa66DPXxRJLjL3NV4NQ5XtfOSLJUFmm9FWsdtpPnmWBHP08M6srxpJalSvj6aTZ5bcNanRyPX/a6swuRCgp2IRJBwS5EIijYhUgEBbsQiaBgFyIRVtL+aTuAvwMwitbG/j53/5qZDQP4HoCr0GoB9Ul3D/f8abO0tITDh8LS1o7dv0HndeXC0luzyhMF8l0RGSRi6+/n0lDfQLiu3fXXv4/O+cmPHqO2hWle765neBO1HTo+QW3bt4WTcna972Y6p1Tkl8HVO3iSz9Qkf7lfORhOKGo61w1PTPFEkhmSDAUAlQaXbWemwlLkps086eatc7w+3fB2LpeeK3E/0OTPbaoefm6e59fpEjleFTzhZiV39jqAv3H3GwB8CMBfmdkNAO4D8Li77wbwePtnIcQVyrLB7u7j7v5s+/EsgIMAtgK4C8AD7V97AMDH18hHIcRl4KL+ZjezqwDcBOBJAKPuv0zuPYXWx3whxBXKioPdzPoA/ADA59z9Xd9PdHcH+aKemd1jZvvNbP/sLC8YIIRYW1YU7GZWQCvQv+PuP2wPnzazsbZ9DEBw18jd97n7XnffG9v8EkKsLcsGu5kZWv3YD7r7Vy4wPQrg7vbjuwE8cvndE0JcLlaS9fa7AD4D4EUze7499nkAXwLwkJl9FsAxAJ9c7kALS3U8fygsG+248RY6r4lwtpmxzB8AaPL0n5nZWWqbmjpLbRuG9wTH77zjI3TOng9cT20P/fBhajPjEsrg4BC1bd0SlpT6Bsp0TlYPry8ADG/ml8jYrhq1TXeHZaPnXuD14sbneEqZF3g7r8HNPItx5JqwVJZFZK2Gcz9e83D7MgA4dIrLg8WMH3OxUgmOL0Qu73ozfH3MNnh24LLB7u4/A8A8/f3l5gshrgz0DTohEkHBLkQiKNiFSAQFuxCJoGAXIhE6WnCy0jC8Pt0dtJ1t8AKAXghLE7kqL4boRJoAgFyO27aM8Wyzf/074cyxrgKXXHbt5G2X/uhPP0Vt33/4H6jt7Cn+vMenw8ULK5VDdE4RXOOZXOS2Q8d41h6qYVnOR3iG4NCmcJFKAGhGKim2vvNF5nWFj9m0cCFKAKhF2opNN/i5ugr8mF15Lr3NWzjLrlbg5/JmeH0bEclWd3YhEkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkQkelt6WG4fWp8PvLIz/jfcP27BwJjm8u8gyknkIkW2sz7782NsKzq665mhQpdF5McPzMOWq7/0Eurz37/CvUxnrfAQBNBHT+vu4NfrxGia9HI8eloTzCEms9Ig3Vc+E5ANAVu1IjWWqVavh5e47PyUcy4rIm7+vnFS5T1sHnFZphHzPjr1m1FvY/0uJQd3YhUkHBLkQiKNiFSAQFuxCJoGAXIhE6uhvfgGEuF04WePzZ1+m8N94Mt4y647dvoHOu2cLb9Bw5HG5NBAC3ffBGausiiQmzVb7D/NA/Pk1tz71yktoW6pFWQpHd4lwh/P7djNTkyxnfRY7tWjeaPAFoieww1xp8jhmvabeESFKI8+eWz5Od7ozf53p6eEJLEdz/Bt9wR8N4qDXIxHqNvy7F/nJw3HL8PLqzC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhGWld7MbDuAv0OrJbMD2OfuXzOzLwL4cwBn2r/6eXd/LHqyfB4bRjYGbZPnuXwyfn4qOP7zF3irm0ZtZ8QTLq1s3EySXQBYFpbDntr/Ep3zDz/9BbUtNXnNNeS59JbLXfx7dGOJJ7t4RJZrRuS1mOTFWigV8vySs4xLmMj4a5aPzMuy8PliTUazyPrmnMuDjUiyUTMiHTLNbvNmLh/3D4Rtb5Yi68Q9+CV1AH/j7s+aWT+AZ8zsx23bV939v67gGEKIdWYlvd7GAYy3H8+a2UEAvGSqEOKK5KI+D5rZVQBuAvBke+heMztgZvebGW8tKoRYd1Yc7GbWB+AHAD7n7jMAvg7gGgB70Lrzf5nMu8fM9pvZ/voib5UshFhbVhTs1qrC/wMA33H3HwKAu59294a7NwF8A0Cwwbq773P3ve6+N9/NG0EIIdaWZYPdzAzANwEcdPevXDA+dsGvfQIA35IWQqw7K9mN/10AnwHwopk93x77PIBPm9ketOS4owD+YrkDmRmVSQoFLjXVK2E54ejpGTpnaf4gtd1283XU1l0eo7bpSlgi+ecn99M5FeeZS7U6l3FKJZ7Z1ozUQVtYCLcSipFFMrKMJ70h0pEJJSJ5xbKyELFZicuU3d28dl2eSH21SEbZ7Pw8tTUiMuVSnb8ug0PhOooAMDoWtvVFCu8tzob/JPbItbGS3fifAQi95FFNXQhxZaFv0AmRCAp2IRJBwS5EIijYhUgEBbsQidDRgpNwR7NOsqhiGUNZWIaqgmc7TcwtUduzr/FCj3cucGll1sNyx4nz/JuBpT6eXVVf4P5Xlrj/PT0RqYm0vYodz3Lcj1ykXVMsg82JjOaR+0shIjfO1Xj2XbXOpTImy8Uy9mIS2nyk9VZfmctr5Y285Vi1Hj7ma6/yrM4CyUasVbl/urMLkQgKdiESQcEuRCIo2IVIBAW7EImgYBciETosvQFgWUPO5Y4sCxfrazqXhRo5XuDv6ASXyu5/iOf3fPT2vcHxIyfPBMcBYKERK0IYkaG6eOHArMhtPaSHWbGby1qLs1y6imWHeUSiKpCMrSzPX7PYubJIUclYH7vFhbmLnhM7V3lomNo2jPKMybPnJqlt6uyp8PhbvCfhtbt2hQ0RSVF3diESQcEuRCIo2IVIBAW7EImgYBciERTsQiRCR6W3LJ9huFwO2ioVLofNL4YzeYoZz/6qR2ShXKS45RNPHaC2IyfD2XLT87xw5OTcIrWRZCcAQG9vJFsuUlSwVAo/t3xEruvq5hllWSQjLl/gx2yQ+0g9InlZxObOfWzU+PpXa+FF7u7iUuTIhg3UNjTC5bVqJHNzqRgpHkn6szXzXD6er4Svq2ZEwtadXYhEULALkQgKdiESQcEuRCIo2IVIhGV3482sC8ATAErt3/++u3/BzHYBeBDABgDPAPiMu0f2lwFvOpbILmIp8raz1AjvthYyvhtc55vI8Bw/Wa6b74IfIwkvuUhyR73Gd5hjikGlUqG2+Uh7ohx5bmyXHgB6i3zXtzuSQJPLcf+LXeHzdffw9a1WeSLM2UmeSNIEn5cvhNdjaKCXzhkdLlPb5s08EWZqntf5m506T21z01PB8fIwP9fZM2eD4/VIMtFK7uxLAD7q7h9Aqz3zHWb2IQB/C+Cr7n4tgPMAPruCYwkh1ollg91bvJMnWGj/cwAfBfD99vgDAD6+Fg4KIS4PK+3PnrU7uE4A+DGANwFMuf+yRelxAFvXxEMhxGVhRcHu7g133wNgG4BbAFy/0hOY2T1mtt/M9tcWeItlIcTaclG78e4+BeCfAHwYQNnsl429twE4Qebsc/e97r630DOwGl+FEKtg2WA3s41mVm4/7gbwMQAH0Qr6P23/2t0AHlkjH4UQl4GVJMKMAXjAzDK03hwecve/N7NXADxoZv8ZwHMAvrncgZrNJpYWw5JSKTM6r4d42azxJJNI1yI0wSWjWCJBk7SbqlcjCRwN/rxiLYhitmYkEYZJb+fPc+lnMrKOA31cohqM1GMbILXwusClvEaTS1d5iyTrlPiLvVQJH7OU569L7Fz1hemIjfs/N3WO2pokWaerxCXRCquTZ5HnRS1t3P0AgJsC44fR+vtdCPErgL5BJ0QiKNiFSAQFuxCJoGAXIhEU7EIkgsUknst+MrMzAI61fxwBEE7d6Szy493Ij3fzq+bHTnffGDJ0NNjfdWKz/e4ebp4mP+SH/LjsfuhjvBCJoGAXIhHWM9j3reO5L0R+vBv58W5+bfxYt7/ZhRCdRR/jhUiEdQl2M7vDzF4zs0Nmdt96+ND246iZvWhmz5vZ/g6e934zmzCzly4YGzazH5vZG+3/h9bJjy+a2Yn2mjxvZnd2wI/tZvZPZvaKmb1sZn/dHu/omkT86OiamFmXmT1lZi+0/fhP7fFdZvZkO26+Z2a84moId+/oPwAZWmWtrgZQBPACgBs67Ufbl6MARtbhvLcBuBnASxeM/RcA97Uf3wfgb9fJjy8C+PcdXo8xADe3H/cDeB3ADZ1ek4gfHV0TAAagr/24AOBJAB8C8BCAT7XH/zuAv7yY467Hnf0WAIfc/bC3Sk8/COCudfBj3XD3JwC8tzbyXWgV7gQ6VMCT+NFx3H3c3Z9tP55FqzjKVnR4TSJ+dBRvcdmLvK5HsG8F8PYFP69nsUoH8CMze8bM7lknH95h1N3H249PARhdR1/uNbMD7Y/5a/7nxIWY2VVo1U94Euu4Ju/xA+jwmqxFkdfUN+hudfebAfwhgL8ys9vW2yGg9c6O1hvRevB1ANeg1SNgHMCXO3ViM+sD8AMAn3P3d1Un7eSaBPzo+Jr4Koq8MtYj2E8A2H7Bz7RY5Vrj7ifa/08AeBjrW3nntJmNAUD7/4n1cMLdT7cvtCaAb6BDa2JmBbQC7Dvu/sP2cMfXJOTHeq1J+9xTuMgir4z1CPanAexu7ywWAXwKwKOddsLMes2s/53HAP4AwEvxWWvKo2gV7gTWsYDnO8HV5hPowJqYmaFVw/Cgu3/lAlNH14T50ek1WbMir53aYXzPbuOdaO10vgngP6yTD1ejpQS8AODlTvoB4LtofRysofW312fR6pn3OIA3APwEwPA6+fFtAC8COIBWsI11wI9b0fqIfgDA8+1/d3Z6TSJ+dHRNAPwWWkVcD6D1xvIfL7hmnwJwCMD/BlC6mOPqG3RCJELqG3RCJIOCXYhEULALkQgKdiESQcEuRCIo2IVIBAW7EImgYBciEf4vt7E0CllzrOkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(examples[0].cpu().permute(1,2,0))\n",
    "img = np.uint8((examples[0]*255).permute(1,2,0).cpu())\n",
    "plt.imsave('examplesvanilla.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103d3d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_image_vanilla[0].cpu().permute(1,2,0))\n",
    "img = np.uint8((adv_image_vanilla[0]*255).permute(1,2,0).cpu())\n",
    "plt.imsave('examplesadv.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9c59126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 5.00000e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23520/253413115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'current lr {:.5e}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'canny'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23520/1489598022.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch, normalizer, method)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# measure data loading time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\machine learning in CS\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 25):\n",
    "\n",
    "    # train for one epoch\n",
    "    print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "    train(train_loader, model, criterion, optimizer, epoch, normalizer, 'canny')\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, model, criterion, normalizer, 'canny')\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "#             'best_prec1': best_prec1,\n",
    "        }, is_best,  filename=os.path.join('', 'checkpoint_canny_64.th'))\n",
    "\n",
    "    save_checkpoint({\n",
    "        'state_dict': model.state_dict(),\n",
    "#         'best_prec1': best_prec1,\n",
    "    }, is_best, filename=os.path.join('', 'resnet32_canny_64.th'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5c83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
